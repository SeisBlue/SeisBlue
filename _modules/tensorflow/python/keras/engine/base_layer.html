<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>tensorflow.python.keras.engine.base_layer &#8212; SeisNN 0.3.0dev documentation</title>
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../_static/jquery.js"></script>
    <script src="../../../../../_static/underscore.js"></script>
    <script src="../../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../../index.html">
          SeisNN</a>
        <span class="navbar-text navbar-version pull-left"><b>0.3.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docker.html">Docker</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorial.html">Tutorial</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebooks.html">Notebooks</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../scripts.html">Scripts</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../package/seisnn.html">seisnn</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
              
                
              
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <h1>Source code for tensorflow.python.keras.engine.base_layer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="c1"># pylint: disable=protected-access</span>
<span class="sd">&quot;&quot;&quot;Contains the base Layer class, from which all layers inherit.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">weakref</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">zip</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">google.protobuf</span> <span class="kn">import</span> <span class="n">json_format</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">tf2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.autograph.core</span> <span class="kn">import</span> <span class="n">ag_ctx</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.autograph.impl</span> <span class="kn">import</span> <span class="n">api</span> <span class="k">as</span> <span class="n">autograph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.distribute</span> <span class="kn">import</span> <span class="n">distribution_strategy_context</span> <span class="k">as</span> <span class="n">ds_context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">def_function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">execute</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">monitoring</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">errors</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">func_graph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="kn">import</span> <span class="n">backend</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="kn">import</span> <span class="n">constraints</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="kn">import</span> <span class="n">initializers</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">base_layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">input_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">keras_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">node</span> <span class="k">as</span> <span class="n">node_module</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.mixed_precision</span> <span class="kn">import</span> <span class="n">autocast_variable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.mixed_precision</span> <span class="kn">import</span> <span class="n">loss_scale_optimizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.mixed_precision</span> <span class="kn">import</span> <span class="n">policy</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.saving.saved_model</span> <span class="kn">import</span> <span class="n">layer_serialization</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="kn">import</span> <span class="n">generic_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="kn">import</span> <span class="n">layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="kn">import</span> <span class="n">tf_inspect</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="kn">import</span> <span class="n">tf_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="kn">import</span> <span class="n">version_utils</span>
<span class="c1"># A module that only depends on `keras.layers` import these from here.</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils.generic_utils</span> <span class="kn">import</span> <span class="n">to_snake_case</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils.tf_utils</span> <span class="kn">import</span> <span class="n">is_tensor_or_tensor_list</span>  <span class="c1"># pylint: disable=unused-import</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.module</span> <span class="kn">import</span> <span class="n">module</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">variables</span> <span class="k">as</span> <span class="n">tf_variables</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.numpy_ops</span> <span class="kn">import</span> <span class="n">np_arrays</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.ragged</span> <span class="kn">import</span> <span class="n">ragged_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="kn">import</span> <span class="n">tf_logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="kn">import</span> <span class="n">data_structures</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="kn">import</span> <span class="n">tracking</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">object_identity</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="kn">import</span> <span class="n">keras_export</span>
<span class="kn">from</span> <span class="nn">tensorflow.tools.docs</span> <span class="kn">import</span> <span class="n">doc_controls</span>

<span class="c1"># pylint: disable=g-inconsistent-quotes</span>
<span class="n">metrics_mod</span> <span class="o">=</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">LazyLoader</span><span class="p">(</span>
    <span class="s2">&quot;metrics_mod&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
    <span class="s2">&quot;tensorflow.python.keras.metrics&quot;</span><span class="p">)</span>
<span class="c1"># pylint: enable=g-inconsistent-quotes</span>

<span class="c1"># Prefix that is added to the TF op layer names.</span>
<span class="n">_TF_OP_LAYER_NAME_PREFIX</span> <span class="o">=</span> <span class="s1">&#39;tf_op_layer_&#39;</span>

<span class="c1"># TODO(mdan): Should we have a single generic type for types that can be passed</span>
<span class="c1"># to tf.cast?</span>
<span class="n">_AUTOCAST_TYPES</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span>
                   <span class="n">ragged_tensor</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">)</span>

<span class="n">keras_layers_gauge</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">BoolGauge</span><span class="p">(</span><span class="s1">&#39;/tensorflow/api/keras/layers&#39;</span><span class="p">,</span>
                                          <span class="s1">&#39;keras layers usage&#39;</span><span class="p">,</span> <span class="s1">&#39;method&#39;</span><span class="p">)</span>
<span class="n">keras_models_gauge</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">BoolGauge</span><span class="p">(</span>
    <span class="s1">&#39;/tensorflow/api/keras/models&#39;</span><span class="p">,</span> <span class="s1">&#39;keras model usage&#39;</span><span class="p">,</span> <span class="s1">&#39;method&#39;</span><span class="p">)</span>
<span class="n">keras_api_gauge</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">BoolGauge</span><span class="p">(</span><span class="s1">&#39;/tensorflow/api/keras&#39;</span><span class="p">,</span>
                                       <span class="s1">&#39;keras api usage&#39;</span><span class="p">,</span> <span class="s1">&#39;method&#39;</span><span class="p">)</span>
<span class="n">keras_premade_model_gauge</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">BoolGauge</span><span class="p">(</span>
    <span class="s1">&#39;/tensorflow/api/keras/premade_models&#39;</span><span class="p">,</span> <span class="s1">&#39;premade keras model usage&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">)</span>


<span class="nd">@keras_export</span><span class="p">(</span><span class="s1">&#39;keras.layers.Layer&#39;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">version_utils</span><span class="o">.</span><span class="n">LayerVersionSelector</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;This is the class from which all layers inherit.</span>

<span class="sd">  A layer is a callable object that takes as input one or more tensors and</span>
<span class="sd">  that outputs one or more tensors. It involves *computation*, defined</span>
<span class="sd">  in the `call()` method, and a *state* (weight variables), defined</span>
<span class="sd">  either in the constructor `__init__()` or in the `build()` method.</span>

<span class="sd">  Users will just instantiate a layer and then treat it as a callable.</span>

<span class="sd">  Arguments:</span>
<span class="sd">    trainable: Boolean, whether the layer&#39;s variables should be trainable.</span>
<span class="sd">    name: String name of the layer.</span>
<span class="sd">    dtype: The dtype of the layer&#39;s computations and weights. Can also be a</span>
<span class="sd">      `tf.keras.mixed_precision.Policy`, which allows the computation and weight</span>
<span class="sd">      dtype to differ. Default of `None` means to use</span>
<span class="sd">      `tf.keras.mixed_precision.global_policy()`, which is a float32 policy</span>
<span class="sd">      unless set to different value.</span>
<span class="sd">    dynamic: Set this to `True` if your layer should only be run eagerly, and</span>
<span class="sd">      should not be used to generate a static computation graph.</span>
<span class="sd">      This would be the case for a Tree-RNN or a recursive network,</span>
<span class="sd">      for example, or generally for any layer that manipulates tensors</span>
<span class="sd">      using Python control flow. If `False`, we assume that the layer can</span>
<span class="sd">      safely be used to generate a static computation graph.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    name: The name of the layer (string).</span>
<span class="sd">    dtype: The dtype of the layer&#39;s weights.</span>
<span class="sd">    variable_dtype: Alias of `dtype`.</span>
<span class="sd">    compute_dtype: The dtype of the layer&#39;s computations. Layers automatically</span>
<span class="sd">      cast inputs to this dtype which causes the computations and output to also</span>
<span class="sd">      be in this dtype. When mixed precision is used with a</span>
<span class="sd">      `tf.keras.mixed_precision.Policy`, this will be different than</span>
<span class="sd">      `variable_dtype`.</span>
<span class="sd">    dtype_policy: The layer&#39;s dtype policy. See the</span>
<span class="sd">      `tf.keras.mixed_precision.Policy` documentation for details.</span>
<span class="sd">    trainable_weights: List of variables to be included in backprop.</span>
<span class="sd">    non_trainable_weights: List of variables that should not be</span>
<span class="sd">      included in backprop.</span>
<span class="sd">    weights: The concatenation of the lists trainable_weights and</span>
<span class="sd">      non_trainable_weights (in this order).</span>
<span class="sd">    trainable: Whether the layer should be trained (boolean), i.e. whether</span>
<span class="sd">      its potentially-trainable weights should be returned as part of</span>
<span class="sd">      `layer.trainable_weights`.</span>
<span class="sd">    input_spec: Optional (list of) `InputSpec` object(s) specifying the</span>
<span class="sd">      constraints on inputs that can be accepted by the layer.</span>

<span class="sd">  We recommend that descendants of `Layer` implement the following methods:</span>

<span class="sd">  * `__init__()`: Defines custom layer attributes, and creates layer state</span>
<span class="sd">    variables that do not depend on input shapes, using `add_weight()`.</span>
<span class="sd">  * `build(self, input_shape)`: This method can be used to create weights that</span>
<span class="sd">    depend on the shape(s) of the input(s), using `add_weight()`. `__call__()`</span>
<span class="sd">    will automatically build the layer (if it has not been built yet) by</span>
<span class="sd">    calling `build()`.</span>
<span class="sd">  * `call(self, *args, **kwargs)`: Called in `__call__` after making sure</span>
<span class="sd">    `build()` has been called. `call()` performs the logic of applying the</span>
<span class="sd">    layer to the input tensors (which should be passed in as argument).</span>
<span class="sd">    Two reserved keyword arguments you can optionally use in `call()` are:</span>
<span class="sd">      - `training` (boolean, whether the call is in</span>
<span class="sd">        inference mode or training mode)</span>
<span class="sd">      - `mask` (boolean tensor encoding masked timesteps in the input, used</span>
<span class="sd">        in RNN layers)</span>
<span class="sd">  * `get_config(self)`: Returns a dictionary containing the configuration used</span>
<span class="sd">    to initialize this layer. If the keys differ from the arguments</span>
<span class="sd">    in `__init__`, then override `from_config(self)` as well.</span>
<span class="sd">    This method is used when saving</span>
<span class="sd">    the layer or a model that contains this layer.</span>

<span class="sd">  Examples:</span>

<span class="sd">  Here&#39;s a basic example: a layer with two variables, `w` and `b`,</span>
<span class="sd">  that returns `y = w . x + b`.</span>
<span class="sd">  It shows how to implement `build()` and `call()`.</span>
<span class="sd">  Variables set as attributes of a layer are tracked as weights</span>
<span class="sd">  of the layers (in `layer.weights`).</span>

<span class="sd">  ```python</span>
<span class="sd">  class SimpleDense(Layer):</span>

<span class="sd">    def __init__(self, units=32):</span>
<span class="sd">        super(SimpleDense, self).__init__()</span>
<span class="sd">        self.units = units</span>

<span class="sd">    def build(self, input_shape):  # Create the state of the layer (weights)</span>
<span class="sd">      w_init = tf.random_normal_initializer()</span>
<span class="sd">      self.w = tf.Variable(</span>
<span class="sd">          initial_value=w_init(shape=(input_shape[-1], self.units),</span>
<span class="sd">                               dtype=&#39;float32&#39;),</span>
<span class="sd">          trainable=True)</span>
<span class="sd">      b_init = tf.zeros_initializer()</span>
<span class="sd">      self.b = tf.Variable(</span>
<span class="sd">          initial_value=b_init(shape=(self.units,), dtype=&#39;float32&#39;),</span>
<span class="sd">          trainable=True)</span>

<span class="sd">    def call(self, inputs):  # Defines the computation from inputs to outputs</span>
<span class="sd">        return tf.matmul(inputs, self.w) + self.b</span>

<span class="sd">  # Instantiates the layer.</span>
<span class="sd">  linear_layer = SimpleDense(4)</span>

<span class="sd">  # This will also call `build(input_shape)` and create the weights.</span>
<span class="sd">  y = linear_layer(tf.ones((2, 2)))</span>
<span class="sd">  assert len(linear_layer.weights) == 2</span>

<span class="sd">  # These weights are trainable, so they&#39;re listed in `trainable_weights`:</span>
<span class="sd">  assert len(linear_layer.trainable_weights) == 2</span>
<span class="sd">  ```</span>

<span class="sd">  Note that the method `add_weight()` offers a shortcut to create weights:</span>

<span class="sd">  ```python</span>
<span class="sd">  class SimpleDense(Layer):</span>

<span class="sd">    def __init__(self, units=32):</span>
<span class="sd">        super(SimpleDense, self).__init__()</span>
<span class="sd">        self.units = units</span>

<span class="sd">    def build(self, input_shape):</span>
<span class="sd">        self.w = self.add_weight(shape=(input_shape[-1], self.units),</span>
<span class="sd">                                 initializer=&#39;random_normal&#39;,</span>
<span class="sd">                                 trainable=True)</span>
<span class="sd">        self.b = self.add_weight(shape=(self.units,),</span>
<span class="sd">                                 initializer=&#39;random_normal&#39;,</span>
<span class="sd">                                 trainable=True)</span>

<span class="sd">    def call(self, inputs):</span>
<span class="sd">        return tf.matmul(inputs, self.w) + self.b</span>
<span class="sd">  ```</span>

<span class="sd">  Besides trainable weights, updated via backpropagation during training,</span>
<span class="sd">  layers can also have non-trainable weights. These weights are meant to</span>
<span class="sd">  be updated manually during `call()`. Here&#39;s a example layer that computes</span>
<span class="sd">  the running sum of its inputs:</span>

<span class="sd">  ```python</span>
<span class="sd">  class ComputeSum(Layer):</span>

<span class="sd">    def __init__(self, input_dim):</span>
<span class="sd">        super(ComputeSum, self).__init__()</span>
<span class="sd">        # Create a non-trainable weight.</span>
<span class="sd">        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),</span>
<span class="sd">                                 trainable=False)</span>

<span class="sd">    def call(self, inputs):</span>
<span class="sd">        self.total.assign_add(tf.reduce_sum(inputs, axis=0))</span>
<span class="sd">        return self.total</span>

<span class="sd">  my_sum = ComputeSum(2)</span>
<span class="sd">  x = tf.ones((2, 2))</span>

<span class="sd">  y = my_sum(x)</span>
<span class="sd">  print(y.numpy())  # [2. 2.]</span>

<span class="sd">  y = my_sum(x)</span>
<span class="sd">  print(y.numpy())  # [4. 4.]</span>

<span class="sd">  assert my_sum.weights == [my_sum.total]</span>
<span class="sd">  assert my_sum.non_trainable_weights == [my_sum.total]</span>
<span class="sd">  assert my_sum.trainable_weights == []</span>
<span class="sd">  ```</span>

<span class="sd">  For more information about creating layers, see the guide</span>
<span class="sd">  [Writing custom layers and models with Keras](</span>
<span class="sd">    https://www.tensorflow.org/guide/keras/custom_layers_and_models)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># See tf.Module for the usage of this property.</span>
  <span class="c1"># The key for _obj_reference_counts_dict is a Trackable, which could be a</span>
  <span class="c1"># variable or layer etc. tf.Module._flatten will fail to flatten the key</span>
  <span class="c1"># since it is trying to convert Trackable to a string. This attribute can be</span>
  <span class="c1"># ignored even after the fix of nest lib, since the trackable object should</span>
  <span class="c1"># already been available as individual attributes. _obj_reference_counts_dict</span>
  <span class="c1"># just contains a copy of them.</span>
  <span class="n">_TF_MODULE_IGNORED_PROPERTIES</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
      <span class="p">(</span><span class="s1">&#39;_obj_reference_counts_dict&#39;</span><span class="p">,),</span>
      <span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="n">_TF_MODULE_IGNORED_PROPERTIES</span>
  <span class="p">))</span>

  <span class="c1"># When loading from a SavedModel, Layers typically can be revived into a</span>
  <span class="c1"># generic Layer wrapper. Sometimes, however, layers may implement methods</span>
  <span class="c1"># that go beyond this wrapper, as in the case of PreprocessingLayers&#39;</span>
  <span class="c1"># `adapt` method. When this is the case, layer implementers can override</span>
  <span class="c1"># must_restore_from_config to return True; layers with this property must</span>
  <span class="c1"># be restored into their actual objects (and will fail if the object is</span>
  <span class="c1"># not available to the restoration code).</span>
  <span class="n">_must_restore_from_config</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">_instrument_layer_creation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_instrumented_keras_api</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_instrumented_keras_layer_class</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_instrumented_keras_model_class</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_disable_keras_instrumentation&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
      <span class="n">keras_api_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_instrumented_keras_api</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_model_for_instrumentation&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">keras_models_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_instrumented_keras_model_class</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">keras_layers_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_instrumented_keras_layer_class</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_instrument_layer_creation</span><span class="p">()</span>

    <span class="c1"># These properties should be set by the user via keyword arguments.</span>
    <span class="c1"># note that &#39;dtype&#39;, &#39;input_shape&#39; and &#39;batch_input_shape&#39;</span>
    <span class="c1"># are only applicable to input layers: do not pass these keywords</span>
    <span class="c1"># to non-input layers.</span>
    <span class="n">allowed_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;input_dim&#39;</span><span class="p">,</span>
        <span class="s1">&#39;input_shape&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_input_shape&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weights&#39;</span><span class="p">,</span>
        <span class="s1">&#39;activity_regularizer&#39;</span><span class="p">,</span>
        <span class="s1">&#39;autocast&#39;</span><span class="p">,</span>
        <span class="s1">&#39;implementation&#39;</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="c1"># Validate optional keyword arguments.</span>
    <span class="n">generic_utils</span><span class="o">.</span><span class="n">validate_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">allowed_kwargs</span><span class="p">)</span>

    <span class="c1"># Mutable properties</span>
    <span class="c1"># Indicates whether the layer&#39;s weights are updated during training</span>
    <span class="c1"># and whether the layer&#39;s updates are run during training.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="c1"># A stateful layer is a layer whose updates are run during inference too,</span>
    <span class="c1"># for instance stateful RNNs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Indicates whether `build` needs to be called upon layer call, to create</span>
    <span class="c1"># the layer&#39;s weights.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Record the build input shape for loading purposes.</span>
    <span class="c1"># TODO(kathywu): Move this to Layer._set_save_spec once cl/290121460 is</span>
    <span class="c1"># submitted.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_build_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_saved_model_inputs_spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Provides information about which inputs are compatible with the layer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_spec</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># `Layer.compute_mask` will be called at the end of `Layer.__call__` if</span>
    <span class="c1"># `Layer.compute_mask` is overridden, or if the `Layer` subclass sets</span>
    <span class="c1"># `self.supports_masking=True`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">is_default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_mask</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_init_set_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;activity_regularizer&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_updates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Object to store all thread local layer properties.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>
    <span class="c1"># A list of zero-argument lambdas which return Tensors, used for variable</span>
    <span class="c1"># regularizers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of symbolic Tensors containing activity regularizers and losses</span>
    <span class="c1"># manually added through `add_loss` in graph-building mode.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of metric instances corresponding to the symbolic metric tensors</span>
    <span class="c1"># added using the `add_metric` API.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Ensures the same metric is not added multiple times in `MirroredStrategy`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="c1"># Both graph and subclassed networks have a dtype policy. For graph</span>
    <span class="c1"># networks, the policy&#39;s compute and variable dtypes are ignored. Such</span>
    <span class="c1"># networks only use the policy if it is a PolicyV1, in which case it uses</span>
    <span class="c1"># the PolicyV1&#39;s loss_scale (Policy does not have a loss_scale). For</span>
    <span class="c1"># subclassed networks, the compute and variable dtypes are used as like any</span>
    <span class="c1"># ordinary layer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Boolean indicating whether the layer automatically casts its inputs to the</span>
    <span class="c1"># layer&#39;s compute_dtype.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_autocast</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;autocast&#39;</span><span class="p">,</span>
                                <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">v2_dtype_behavior_enabled</span><span class="p">())</span>

    <span class="c1"># Dependencies tracked via attribute assignment.</span>
    <span class="c1"># All layers in order of horizontal graph traversal.</span>
    <span class="c1"># Entries are unique. For models includes input and output layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="p">[])</span>

    <span class="c1"># These lists will be filled via successive calls</span>
    <span class="c1"># to self._add_inbound_node().</span>
    <span class="c1"># Used in symbolic mode only, only in conjunction with graph-networks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes_value</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes_value</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_init_call_fn_args</span><span class="p">()</span>

    <span class="c1"># Whether the `call` method can be used to build a TF graph without issues.</span>
    <span class="c1"># This attribute has no effect if the model is created using the Functional</span>
    <span class="c1"># API. Instead, `model.dynamic` is determined based on the internal layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>

    <span class="c1"># Manage input shape information if passed.</span>
    <span class="k">if</span> <span class="s1">&#39;input_dim&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="c1"># Backwards compatibility: alias &#39;input_dim&#39; to &#39;input_shape&#39;.</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;input_shape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;input_dim&#39;</span><span class="p">],)</span>
    <span class="k">if</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="s1">&#39;batch_input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="c1"># In this case we will later create an input layer</span>
      <span class="c1"># to insert before the current layer</span>
      <span class="k">if</span> <span class="s1">&#39;batch_input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;batch_input_shape&#39;</span><span class="p">])</span>
      <span class="k">elif</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;batch_size&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;input_shape&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batch_input_shape</span> <span class="o">=</span> <span class="n">batch_input_shape</span>

    <span class="c1"># Manage initial weight values if passed.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># Whether the layer will track any layers that is set as attribute on itself</span>
    <span class="c1"># as sub-layers, the weights from the sub-layers will be included in the</span>
    <span class="c1"># parent layer&#39;s variables() as well.</span>
    <span class="c1"># Default to True, which means auto tracking is turned on. Certain subclass</span>
    <span class="c1"># might want to turn it off, like Sequential model.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_auto_track_sub_layers</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># For backwards compat reasons, most built-in layers do not guarantee</span>
    <span class="c1"># That they will 100% preserve the structure of input args when saving</span>
    <span class="c1"># / loading configs. E.g. they may un-nest an arg that is</span>
    <span class="c1"># a list with one element.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_preserve_input_structure_in_config</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="nd">@generic_utils</span><span class="o">.</span><span class="n">default</span>
  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates the variables of the layer (optional, for subclass implementers).</span>

<span class="sd">    This is a method that implementers of subclasses of `Layer` or `Model`</span>
<span class="sd">    can override if they need a state-creation step in-between</span>
<span class="sd">    layer instantiation and layer call.</span>

<span class="sd">    This is typically used to create the weights of `Layer` subclasses.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      input_shape: Instance of `TensorShape`, or list of instances of</span>
<span class="sd">        `TensorShape` if the layer expects a list of inputs</span>
<span class="sd">        (one instance per input).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Only record the build input shapes of overridden build methods.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">,</span> <span class="s1">&#39;_is_default&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_build_input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
    <span class="sd">&quot;&quot;&quot;This is where the layer&#39;s logic lives.</span>

<span class="sd">    Note here that `call()` method in `tf.keras` is little bit different</span>
<span class="sd">    from `keras` API. In `keras` API, you can pass support masking for</span>
<span class="sd">    layers as additional arguments. Whereas `tf.keras` has `compute_mask()`</span>
<span class="sd">    method to support masking.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs: Input tensor, or list/tuple of input tensors.</span>
<span class="sd">        **kwargs: Additional keyword arguments. Currently unused.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor or list/tuple of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inputs</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">_add_trackable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trackable_object</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a Trackable object to this layer&#39;s state.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      trackable_object: The tf.tracking.Trackable object to add.</span>
<span class="sd">      trainable: Boolean, whether the variable should be part of the layer&#39;s</span>
<span class="sd">        &quot;trainable_variables&quot; (e.g. variables, biases) or</span>
<span class="sd">        &quot;non_trainable_variables&quot; (e.g. BatchNorm mean and variance).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The TrackableWeightHandler used to track this object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handler</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">(</span><span class="n">trackable_object</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">handler</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">synchronization</span><span class="o">=</span><span class="n">tf_variables</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                 <span class="n">aggregation</span><span class="o">=</span><span class="n">tf_variables</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a new variable to the layer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      name: Variable name.</span>
<span class="sd">      shape: Variable shape. Defaults to scalar if unspecified.</span>
<span class="sd">      dtype: The type of the variable. Defaults to `self.dtype`.</span>
<span class="sd">      initializer: Initializer instance (callable).</span>
<span class="sd">      regularizer: Regularizer instance (callable).</span>
<span class="sd">      trainable: Boolean, whether the variable should be part of the layer&#39;s</span>
<span class="sd">        &quot;trainable_variables&quot; (e.g. variables, biases)</span>
<span class="sd">        or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean and variance).</span>
<span class="sd">        Note that `trainable` cannot be `True` if `synchronization`</span>
<span class="sd">        is set to `ON_READ`.</span>
<span class="sd">      constraint: Constraint instance (callable).</span>
<span class="sd">      use_resource: Whether to use `ResourceVariable`.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize. If `synchronization` is set to `ON_READ`,</span>
<span class="sd">        `trainable` must not be set to `True`.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      **kwargs: Additional keyword arguments. Accepted values are `getter`,</span>
<span class="sd">        `collections`, `experimental_autocast` and `caching_device`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The variable created.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: When giving unsupported dtype and no initializer or when</span>
<span class="sd">        trainable has been set to True with synchronization set as `ON_READ`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;partitioner&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Ignored.</span>
    <span class="c1"># Validate optional keyword arguments.</span>
    <span class="k">for</span> <span class="n">kwarg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">kwarg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span>
                       <span class="s1">&#39;caching_device&#39;</span><span class="p">,</span> <span class="s1">&#39;getter&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword argument:&#39;</span><span class="p">,</span> <span class="n">kwarg</span><span class="p">)</span>
    <span class="n">collections_arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate an</span>
    <span class="c1"># AutoCastVariable should never be created.</span>
    <span class="n">autocast</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="c1"># See the docstring for tf.Variable about the details for caching_device.</span>
    <span class="n">caching_device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">backend</span><span class="o">.</span><span class="n">floatx</span><span class="p">()</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">variable_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># The policy is &quot;_infer&quot;, so we infer the policy from the variable dtype.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>
    <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span>
    <span class="n">constraint</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">synchronization</span> <span class="o">==</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">ON_READ</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;Synchronization value can be set to &#39;</span>
            <span class="s1">&#39;VariableSynchronization.ON_READ only for non-trainable variables. &#39;</span>
            <span class="s1">&#39;You have specified trainable=True and &#39;</span>
            <span class="s1">&#39;synchronization=VariableSynchronization.ON_READ.&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Set trainable to be false when variable is to be synced on read.</span>
        <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="n">trainable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Initialize variable when no initializer provided</span>
    <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span>
      <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">)</span>
      <span class="c1"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span>
      <span class="c1"># If dtype is DT_BOOL, provide a default value `FALSE`</span>
      <span class="k">elif</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_unsigned</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_bool</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
      <span class="c1"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;An initializer for variable </span><span class="si">%s</span><span class="s1"> of type </span><span class="si">%s</span><span class="s1"> is required&#39;</span>
                         <span class="s1">&#39; for layer </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="n">getter</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;getter&#39;</span><span class="p">,</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">make_variable</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">autocast</span> <span class="ow">and</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">compute_dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">variable_dtype</span>
        <span class="ow">and</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">):</span>
      <span class="n">old_getter</span> <span class="o">=</span> <span class="n">getter</span>
      <span class="c1"># Wrap variable constructor to return an AutoCastVariable.</span>
      <span class="k">def</span> <span class="nf">getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=function-redefined</span>
        <span class="n">variable</span> <span class="o">=</span> <span class="n">old_getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">create_autocast_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
      <span class="c1"># Also the caching_device does not work with the mixed precision API,</span>
      <span class="c1"># disable it if it is specified.</span>
      <span class="c1"># TODO(b/142020079): Reenable it once the bug is fixed.</span>
      <span class="k">if</span> <span class="n">caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tf_logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`caching_device` does not work with mixed precision &#39;</span>
                        <span class="s1">&#39;API. Ignoring user specified `caching_device`.&#39;</span><span class="p">)</span>
        <span class="n">caching_device</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">variable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
        <span class="c1"># TODO(allenl): a `make_variable` equivalent should be added as a</span>
        <span class="c1"># `Trackable` method.</span>
        <span class="n">getter</span><span class="o">=</span><span class="n">getter</span><span class="p">,</span>
        <span class="c1"># Manage errors in Layer rather than Trackable.</span>
        <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># TODO(fchollet): in the future, this should be handled at the</span>
      <span class="c1"># level of variable creation, and weight regularization losses</span>
      <span class="c1"># should be variable attributes.</span>
      <span class="n">name_in_scope</span> <span class="o">=</span> <span class="n">variable</span><span class="o">.</span><span class="n">name</span><span class="p">[:</span><span class="n">variable</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span><span class="n">name_in_scope</span><span class="p">,</span>
                                         <span class="n">variable</span><span class="p">,</span>
                                         <span class="n">regularizer</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable</span><span class="p">:</span>
        <span class="n">backend</span><span class="o">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">backend</span><span class="o">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">variable</span>

  <span class="nd">@generic_utils</span><span class="o">.</span><span class="n">default</span>
  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the config of the layer.</span>

<span class="sd">    A layer config is a Python dictionary (serializable)</span>
<span class="sd">    containing the configuration of a layer.</span>
<span class="sd">    The same layer can be reinstantiated later</span>
<span class="sd">    (without its trained weights) from this configuration.</span>

<span class="sd">    The config of a layer does not include connectivity</span>
<span class="sd">    information, nor the layer class name. These are handled</span>
<span class="sd">    by `Network` (one layer of abstraction above).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Python dictionary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_args</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span><span class="o">.</span><span class="n">args</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="s1">&#39;trainable&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_batch_input_shape&#39;</span><span class="p">):</span>
      <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_input_shape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_input_shape</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;dynamic&#39;</span><span class="p">):</span>
      <span class="c1"># Only include `dynamic` in the `config` if it is `True`</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span><span class="p">:</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dynamic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span>
      <span class="k">elif</span> <span class="s1">&#39;dynamic&#39;</span> <span class="ow">in</span> <span class="n">all_args</span><span class="p">:</span>
        <span class="n">all_args</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;dynamic&#39;</span><span class="p">)</span>
    <span class="n">expected_args</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="c1"># Finds all arguments in the `__init__` that are not in the config:</span>
    <span class="n">extra_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">arg</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">all_args</span> <span class="k">if</span> <span class="n">arg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">expected_args</span><span class="p">]</span>
    <span class="c1"># Check that either the only argument in the `__init__` is  `self`,</span>
    <span class="c1"># or that `get_config` has been overridden:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">extra_args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_config</span><span class="p">,</span> <span class="s1">&#39;_is_default&#39;</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Layer </span><span class="si">%s</span><span class="s1"> has arguments in `__init__` and &#39;</span>
                                <span class="s1">&#39;therefore must override `get_config`.&#39;</span> <span class="o">%</span>
                                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">config</span>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a layer from its config.</span>

<span class="sd">    This method is the reverse of `get_config`,</span>
<span class="sd">    capable of instantiating the same layer from the config</span>
<span class="sd">    dictionary. It does not handle layer connectivity</span>
<span class="sd">    (handled by Network), nor weights (handled by `set_weights`).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        config: A Python dictionary, typically the</span>
<span class="sd">            output of get_config.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A layer instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="sd">    If the layer has not been built, this method will call `build` on the</span>
<span class="sd">    layer. This assumes that the layer will later be used with inputs that</span>
<span class="sd">    match the input shape provided here.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input_shape: Shape tuple (tuple of integers)</span>
<span class="sd">            or list of shape tuples (one per output tensor of the layer).</span>
<span class="sd">            Shape tuples can include None for free dimensions,</span>
<span class="sd">            instead of an integer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An input shape tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># In this case we build the model first in order to do shape inference.</span>
      <span class="c1"># This is acceptable because the framework only calls</span>
      <span class="c1"># `compute_output_shape` on shape values that the layer would later be</span>
      <span class="c1"># built for. It would however cause issues in case a user attempts to</span>
      <span class="c1"># use `compute_output_shape` manually with shapes that are incompatible</span>
      <span class="c1"># with the shape the Layer will be called on (these users will have to</span>
      <span class="c1"># implement `compute_output_shape` themselves).</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_scratch_graph&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_make_placeholder_like</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
          <span class="n">ph</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
          <span class="n">ph</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="k">return</span> <span class="n">ph</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_make_placeholder_like</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span>
              <span class="ne">NotImplementedError</span><span class="p">(</span>
                  <span class="s1">&#39;We could not automatically infer the static shape of the &#39;</span>
                  <span class="s1">&#39;layer</span><span class="se">\&#39;</span><span class="s1">s output. Please implement the &#39;</span>
                  <span class="s1">&#39;`compute_output_shape` method on your layer (</span><span class="si">%s</span><span class="s1">).&#39;</span> <span class="o">%</span>
                  <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span> <span class="n">e</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s1">&#39;Please run in eager mode or implement the `compute_output_shape` &#39;</span>
        <span class="s1">&#39;method on your layer (</span><span class="si">%s</span><span class="s1">).&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">compute_output_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the output tensor signature of the layer based on the inputs.</span>

<span class="sd">    Unlike a TensorShape object, a TensorSpec object contains both shape</span>
<span class="sd">    and dtype information for a tensor. This method allows layers to provide</span>
<span class="sd">    output dtype information if it is different from the input dtype.</span>
<span class="sd">    For any layer that doesn&#39;t implement this function,</span>
<span class="sd">    the framework will fall back to use `compute_output_shape`, and will</span>
<span class="sd">    assume that the output dtype matches the input dtype.</span>

<span class="sd">    Args:</span>
<span class="sd">      input_signature: Single TensorSpec or nested structure of TensorSpec</span>
<span class="sd">        objects, describing a candidate input for the layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Single TensorSpec or nested structure of TensorSpec objects, describing</span>
<span class="sd">        how the layer would transform the provided input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If input_signature contains a non-TensorSpec object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">check_type_return_shape</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tensor_spec</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s1">&#39;Only TensorSpec signature types are supported, &#39;</span>
            <span class="s1">&#39;but saw signature signature entry: </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">check_type_return_shape</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">)</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">input_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)]</span>
      <span class="c1"># Default behavior when self.dtype is None, is to use the first input&#39;s</span>
      <span class="c1"># dtype.</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_dtypes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">tensor_spec</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">s</span><span class="p">),</span>
        <span class="n">output_shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_keras_tensor_symbolic_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span><span class="p">:</span>
      <span class="c1"># We will use static shape inference to return symbolic tensors</span>
      <span class="c1"># matching the specifications of the layer outputs.</span>
      <span class="c1"># Since `self.dynamic` is True, we will never attempt to</span>
      <span class="c1"># run the underlying TF graph (which is disconnected).</span>
      <span class="c1"># TODO(fchollet): consider py_func as an alternative, which</span>
      <span class="c1"># would enable us to run the underlying graph if needed.</span>
      <span class="n">input_signature</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tensor_spec</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">inputs</span><span class="p">)</span>
      <span class="n">output_signature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_signature</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">keras_tensor</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">,</span> <span class="n">output_signature</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_output_signature</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_infer_output_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TODO(kaftan): Docstring.&quot;&quot;&quot;</span>

    <span class="n">call_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span>
    <span class="c1"># Wrapping `call` function in autograph to allow for dynamic control</span>
    <span class="c1"># flow and control dependencies in call. We are limiting this to</span>
    <span class="c1"># subclassed layers as autograph is strictly needed only for</span>
    <span class="c1"># subclassed layers and models.</span>
    <span class="c1"># tf_convert will respect the value of autograph setting in the</span>
    <span class="c1"># enclosing tf.function, if any.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_subclassed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)):</span>
      <span class="n">call_fn</span> <span class="o">=</span> <span class="n">autograph</span><span class="o">.</span><span class="n">tf_convert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">())</span>

    <span class="c1"># We enter a scratch graph and build placeholder inputs inside of it that</span>
    <span class="c1"># match the input args.</span>
    <span class="c1"># We then call the layer inside of the scratch graph to identify the</span>
    <span class="c1"># output signatures, then we build KerasTensors corresponding to those</span>
    <span class="c1"># outputs.</span>
    <span class="n">scratch_graph</span> <span class="o">=</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_scratch_graph&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">scratch_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensor_to_placeholder</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">args</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensor_to_placeholder</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
      <span class="n">kwargs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensor_to_placeholder</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
      <span class="n">input_masks</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensor_to_placeholder</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">)</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_cast_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="p">()):</span>
        <span class="k">with</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">enable_auto_cast_variables</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span><span class="p">):</span>
          <span class="c1"># Build layer if applicable (if the `build` method has been</span>
          <span class="c1"># overridden).</span>
          <span class="c1"># TODO(kaftan): do we maybe_build here, or have we already done it?</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">call_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_handle_activity_regularization</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">,</span>
                              <span class="n">build_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensor_from_tensor</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_set_inputs&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="c1"># TODO(kaftan): figure out if we ned to do this at all</span>
      <span class="c1"># Subclassed network: explicitly set metadata normally set by</span>
      <span class="c1"># a call to self._set_inputs().</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_set_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">scratch_graph</span>
    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@generic_utils</span><span class="o">.</span><span class="n">default</span>
  <span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
    <span class="sd">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs: Tensor or list of tensors.</span>
<span class="sd">        mask: Tensor or list of tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None or a tensor (or list of tensors,</span>
<span class="sd">            one per output tensor of the layer).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39; does not support masking, &#39;</span>
                        <span class="s1">&#39;but was passed an input_mask: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
      <span class="c1"># masking not explicitly supported: return None as mask.</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="c1"># if masking is explicitly supported, by default</span>
    <span class="c1"># carry over the input mask</span>
    <span class="k">return</span> <span class="n">mask</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps `call`, applying pre- and post-processing steps.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      *args: Positional arguments to be passed to `self.call`.</span>
<span class="sd">      **kwargs: Keyword arguments to be passed to `self.call`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor(s).</span>

<span class="sd">    Note:</span>
<span class="sd">      - The following optional keyword arguments are reserved for specific uses:</span>
<span class="sd">        * `training`: Boolean scalar tensor of Python boolean indicating</span>
<span class="sd">          whether the `call` is meant for training or inference.</span>
<span class="sd">        * `mask`: Boolean input mask.</span>
<span class="sd">      - If the layer&#39;s `call` method takes a `mask` argument (as some Keras</span>
<span class="sd">        layers do), its default value will be set to the mask generated</span>
<span class="sd">        for `inputs` by the previous layer (if `input` did come from</span>
<span class="sd">        a layer that generated a corresponding mask, i.e. if it came from</span>
<span class="sd">        a Keras layer with masking support.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the layer&#39;s `call` method returns None (an invalid value).</span>
<span class="sd">      RuntimeError: if `super().__init__()` was not called in the constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_thread_local&#39;</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s1">&#39;You must call `super().__init__()` in the layer constructor.&#39;</span><span class="p">)</span>

    <span class="c1"># `inputs` (the first arg in the method spec) is special cased in</span>
    <span class="c1"># layer call due to historical reasons.</span>
    <span class="c1"># This special casing currently takes the form of:</span>
    <span class="c1"># - &#39;inputs&#39; must be explicitly passed. A layer cannot have zero arguments,</span>
    <span class="c1">#   and inputs cannot have been provided via the default value of a kwarg.</span>
    <span class="c1"># - numpy/scalar values in `inputs` get converted to tensors</span>
    <span class="c1"># - implicit masks / mask metadata are only collected from &#39;inputs`</span>
    <span class="c1"># - Layers are built using shape info from &#39;inputs&#39; only</span>
    <span class="c1"># - input_spec compatibility is only checked against `inputs`</span>
    <span class="c1"># - mixed precision casting (autocast) is only applied to `inputs`,</span>
    <span class="c1">#   not to any other argument.</span>
    <span class="c1"># - setting the SavedModel saving spec.</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_out_first_arg</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Functional Model construction mode is invoked when `Layer`s are called on</span>
    <span class="c1"># symbolic `KerasTensor`s, i.e.:</span>
    <span class="c1"># &gt;&gt; inputs = tf.keras.Input(10)</span>
    <span class="c1"># &gt;&gt; outputs = MyLayer()(inputs)  # Functional construction mode.</span>
    <span class="c1"># &gt;&gt; model = tf.keras.Model(inputs, outputs)</span>
    <span class="k">if</span> <span class="n">_in_functional_construction_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functional_construction_call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span>
                                                <span class="n">input_list</span><span class="p">)</span>

    <span class="c1"># Maintains info about the `Layer.call` stack.</span>
    <span class="n">call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">call_context</span><span class="p">()</span>

    <span class="c1"># Accept NumPy and scalar inputs by converting to Tensors.</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span>
        <span class="n">np_arrays</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_convert_numpy_or_python_types</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Handle `mask` propagation from previous layer to current layer. Masks can</span>
    <span class="c1"># be propagated explicitly via the `mask` argument, or implicitly via</span>
    <span class="c1"># setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed</span>
    <span class="c1"># explicitly take priority.</span>
    <span class="n">input_masks</span><span class="p">,</span> <span class="n">mask_is_implicit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_masks</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_mask_arg</span> <span class="ow">and</span> <span class="n">mask_is_implicit</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;mask&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_masks</span>

    <span class="c1"># Training mode for `Layer.call` is set via (in order of priority):</span>
    <span class="c1"># (1) The `training` argument passed to this `Layer.call`, if it is not None</span>
    <span class="c1"># (2) The training mode of an outer `Layer.call`.</span>
    <span class="c1"># (3) The default mode set by `tf.keras.backend.set_learning_phase` (if set)</span>
    <span class="c1"># (4) Any non-None default value for `training` specified in the call</span>
    <span class="c1">#  signature</span>
    <span class="c1"># (5) False (treating the layer as if it&#39;s in inference)</span>
    <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">training_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">call_context</span><span class="p">)</span>

    <span class="c1"># Losses are cleared for all sublayers on the outermost `Layer.call`.</span>
    <span class="c1"># Losses are not cleared on inner `Layer.call`s, because sublayers can be</span>
    <span class="c1"># called multiple times.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">call_context</span><span class="o">.</span><span class="n">in_call</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_clear_losses</span><span class="p">()</span>

    <span class="n">eager</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">call_context</span><span class="o">.</span><span class="n">enter</span><span class="p">(</span>
        <span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">build_graph</span><span class="o">=</span><span class="ow">not</span> <span class="n">eager</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training_mode</span><span class="p">):</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_autocast</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_cast_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">)</span>

      <span class="n">input_spec</span><span class="o">.</span><span class="n">assert_input_compatibility</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">eager</span><span class="p">:</span>
        <span class="n">call_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span>
        <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="p">()</span>  <span class="c1"># Avoid autoincrementing.</span>
        <span class="n">call_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_autographed_call</span><span class="p">()</span>

      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope_v2</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">enable_auto_cast_variables</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span><span class="p">):</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">call_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle_activity_regularization</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">,</span> <span class="ow">not</span> <span class="n">eager</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_saved_model_inputs_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_set_save_spec</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>

  <span class="k">def</span> <span class="nf">_functional_construction_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">):</span>
    <span class="n">call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">call_context</span><span class="p">()</span>

    <span class="c1"># Accept NumPy and scalar inputs by converting to Tensors.</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span>
        <span class="n">np_arrays</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_convert_non_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># Don&#39;t call `ops.convert_to_tensor` on all `inputs` because</span>
        <span class="c1"># `SparseTensors` can&#39;t be converted to `Tensor`.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">np_arrays</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
          <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_v2_with_dispatch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_convert_non_tensor</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Handle `mask` propagation from previous layer to current layer. Masks can</span>
    <span class="c1"># be propagated explicitly via the `mask` argument, or implicitly via</span>
    <span class="c1"># setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed</span>
    <span class="c1"># explicitly take priority.</span>
    <span class="n">mask_arg_passed_by_framework</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">input_masks</span><span class="p">,</span> <span class="n">mask_is_implicit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_masks</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_mask_arg</span> <span class="ow">and</span> <span class="n">mask_is_implicit</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;mask&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_masks</span>
      <span class="n">mask_arg_passed_by_framework</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># If `training` argument is None or not explicitly passed,</span>
    <span class="c1"># propagate `training` value from this layer&#39;s calling layer.</span>
    <span class="n">training_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">training_arg_passed_by_framework</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Priority 1: `training` was explicitly passed a non-None value.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_arg_was_passed</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
      <span class="n">training_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_call_arg_value</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">training_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Priority 2: `training` was passed to a parent layer.</span>
      <span class="k">if</span> <span class="n">call_context</span><span class="o">.</span><span class="n">training</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">training_value</span> <span class="o">=</span> <span class="n">call_context</span><span class="o">.</span><span class="n">training</span>
      <span class="c1"># Priority 3: `learning_phase()` has been set.</span>
      <span class="k">elif</span> <span class="n">backend</span><span class="o">.</span><span class="n">global_learning_phase_is_set</span><span class="p">():</span>
        <span class="n">training_value</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">learning_phase</span><span class="p">()</span>
        <span class="c1"># Force the training_value to be bool type which matches to the contract</span>
        <span class="c1"># for layer/model call args.</span>
        <span class="k">if</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">training_value</span><span class="p">):</span>
          <span class="n">training_value</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">training_value</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">training_value</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">training_value</span><span class="p">)</span>
      <span class="c1"># Priority 4: trace layer with the default training argument specified</span>
      <span class="c1"># in the `call` signature (or in inference mode if the `call` signature</span>
      <span class="c1"># specifies no non-None default).</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">training_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_training_arg</span>
      <span class="c1"># In cases (2), (3), (4) the training argument is passed automatically</span>
      <span class="c1"># by the framework, and will not be hard-coded into the model.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span><span class="p">:</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_call_arg_value</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">training_value</span><span class="p">,</span>
                                                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="n">training_arg_passed_by_framework</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensors_enabled</span><span class="p">():</span>
      <span class="k">with</span> <span class="n">call_context</span><span class="o">.</span><span class="n">enter</span><span class="p">(</span>
          <span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">build_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training_value</span><span class="p">):</span>
        <span class="c1"># Check input assumptions set after layer building, e.g. input shape.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keras_tensor_symbolic_call</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;A layer</span><span class="se">\&#39;</span><span class="s1">s `call` method should return a &#39;</span>
                           <span class="s1">&#39;Tensor or a list of Tensors, not None &#39;</span>
                           <span class="s1">&#39;(layer: &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;).&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">training_arg_passed_by_framework</span><span class="p">:</span>
          <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_call_arg_value</span><span class="p">(</span>
              <span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">pop_kwarg_if_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask_arg_passed_by_framework</span><span class="p">:</span>
          <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">)</span>
        <span class="c1"># Node connectivity does not special-case the first argument.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_connectivity_metadata</span><span class="p">((</span><span class="n">inputs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span>
                                                  <span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="c1"># Only create Keras history if at least one tensor originates from a</span>
    <span class="c1"># `keras.Input`. Otherwise this Layer may be being used outside the Keras</span>
    <span class="c1"># framework.</span>
    <span class="c1"># TODO(kaftan): make this not special case inputs</span>
    <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">needs_keras_history</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_keras_history</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">call_context</span><span class="o">.</span><span class="n">enter</span><span class="p">(</span>
        <span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">build_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training_value</span><span class="p">):</span>
      <span class="c1"># Symbolic execution on symbolic tensors. We will attempt to build</span>
      <span class="c1"># the corresponding TF subgraph inside `backend.get_graph()`</span>
      <span class="c1"># TODO(reedwm): We should assert input compatibility after the inputs</span>
      <span class="c1"># are casted, not before.</span>
      <span class="n">input_spec</span><span class="o">.</span><span class="n">assert_input_compatibility</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span>
      <span class="c1"># Use `self._name_scope()` to avoid auto-incrementing the name.</span>
      <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="p">()):</span>
        <span class="c1"># Build layer if applicable (if the `build` method has been</span>
        <span class="c1"># overridden).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">cast_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_cast_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span><span class="p">:</span>
          <span class="c1"># Wrapping `call` function in autograph to allow for dynamic control</span>
          <span class="c1"># flow and control dependencies in call. We are limiting this to</span>
          <span class="c1"># subclassed layers as autograph is strictly needed only for</span>
          <span class="c1"># subclassed layers and models.</span>
          <span class="c1"># tf_convert will respect the value of autograph setting in the</span>
          <span class="c1"># enclosing tf.function, if any.</span>
          <span class="k">if</span> <span class="p">(</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_subclassed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">and</span>
              <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)):</span>
            <span class="n">call_fn</span> <span class="o">=</span> <span class="n">autograph</span><span class="o">.</span><span class="n">tf_convert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span>
                                           <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">())</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">call_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span>

          <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">enable_auto_cast_variables</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span><span class="p">):</span>
              <span class="n">outputs</span> <span class="o">=</span> <span class="n">call_fn</span><span class="p">(</span><span class="n">cast_inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

          <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">OperatorNotAllowedInGraphError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;You are attempting to use Python control &#39;</span>
                            <span class="s1">&#39;flow in a layer that was not declared to be &#39;</span>
                            <span class="s1">&#39;dynamic. Pass `dynamic=True` to the class &#39;</span>
                            <span class="s1">&#39;constructor.</span><span class="se">\n</span><span class="s1">Encountered error:</span><span class="se">\n</span><span class="s1">&quot;&quot;&quot;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span>
                            <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&quot;&quot;&quot;&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># We will use static shape inference to return symbolic tensors</span>
          <span class="c1"># matching the specifications of the layer outputs.</span>
          <span class="c1"># Since `self.dynamic` is True, we will never attempt to</span>
          <span class="c1"># run the underlying TF graph (which is disconnected).</span>
          <span class="c1"># TODO(fchollet): consider py_func as an alternative, which</span>
          <span class="c1"># would enable us to run the underlying graph if needed.</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_symbolic_call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;A layer</span><span class="se">\&#39;</span><span class="s1">s `call` method should return a &#39;</span>
                           <span class="s1">&#39;Tensor or a list of Tensors, not None &#39;</span>
                           <span class="s1">&#39;(layer: &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;).&#39;</span><span class="p">)</span>
        <span class="c1"># TODO(kaftan): This should be &#39;any&#39; and check all args</span>
        <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">have_all_keras_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">training_arg_passed_by_framework</span><span class="p">:</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_call_arg_value</span><span class="p">(</span>
                <span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">pop_kwarg_if_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">mask_arg_passed_by_framework</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">)</span>
          <span class="c1"># Node connectivity does not special-case the first argument.</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_connectivity_metadata</span><span class="p">((</span><span class="n">inputs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span>
                                                    <span class="n">outputs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_handle_activity_regularization</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_set_inputs&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
          <span class="c1"># Subclassed network: explicitly set metadata normally set by</span>
          <span class="c1"># a call to self._set_inputs().</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_set_inputs</span><span class="p">(</span><span class="n">cast_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="k">def</span> <span class="nf">_set_training_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">call_context</span><span class="p">):</span>
    <span class="n">training_mode</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span><span class="p">:</span>
      <span class="c1"># (1) `training` was passed to this `Layer.call`.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_arg_was_passed</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">training_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_call_arg_value</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
      <span class="c1"># If no `training` arg was passed, or `None` was explicitly passed,</span>
      <span class="c1"># the framework will make a decision about the training mode is.</span>
      <span class="k">if</span> <span class="n">training_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">call_ctx_training</span> <span class="o">=</span> <span class="n">call_context</span><span class="o">.</span><span class="n">training</span>
        <span class="c1"># (2) `training` mode is inferred from an outer `Layer.call`.</span>
        <span class="k">if</span> <span class="n">call_ctx_training</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">training_mode</span> <span class="o">=</span> <span class="n">call_ctx_training</span>
        <span class="c1"># (3) User set `tf.keras.backend.set_learning_phase`.</span>
        <span class="k">elif</span> <span class="n">backend</span><span class="o">.</span><span class="n">global_learning_phase_is_set</span><span class="p">():</span>
          <span class="n">training_mode</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">learning_phase</span><span class="p">()</span>
          <span class="c1"># Ensure value is a `bool` or `tf.bool`.</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">pass</span>
          <span class="k">elif</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">training_mode</span><span class="p">):</span>
            <span class="n">training_mode</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">training_mode</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">training_mode</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">training_mode</span><span class="p">)</span>
        <span class="c1"># (4) We default to using `call`&#39;s default value for `training`,</span>
        <span class="c1"># or treating the layer as if it is in inference if no non-None default</span>
        <span class="c1"># is specified in the `call` signature.</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">training_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_training_arg</span>

        <span class="c1"># For case (2), (3), (4) `training` arg is passed by framework.</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_call_arg_value</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">training_mode</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                                <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="s1">&#39;training&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="c1"># `training` was passed to this `Layer` but is not needed for</span>
        <span class="c1"># `Layer.call`. It will set the default mode for inner `Layer.call`s.</span>
        <span class="n">training_mode</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Grab the current `training` mode from any outer `Layer.call`.</span>
        <span class="n">training_mode</span> <span class="o">=</span> <span class="n">call_context</span><span class="o">.</span><span class="n">training</span>

    <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">training_mode</span>

  <span class="k">def</span> <span class="nf">_autographed_call</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Wrapping `call` function in autograph to allow for dynamic control</span>
    <span class="c1"># flow and control dependencies in call. We are limiting this to</span>
    <span class="c1"># subclassed layers as autograph is strictly needed only for</span>
    <span class="c1"># subclassed layers and models.</span>
    <span class="c1"># tf_convert will respect the value of autograph setting in the</span>
    <span class="c1"># enclosing tf.function, if any.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_subclassed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)):</span>
      <span class="k">return</span> <span class="n">autograph</span><span class="o">.</span><span class="n">tf_convert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The dtype of the layer weights.</span>

<span class="sd">    This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless</span>
<span class="sd">    mixed precision is used, this is the same as `Layer.compute_dtype`, the</span>
<span class="sd">    dtype of the layer&#39;s computations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">variable_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Name of the layer (string), set in the constructor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">supports_masking</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Whether this layer supports computing a mask using `compute_mask`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span>

  <span class="nd">@supports_masking</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">supports_masking</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dynamic</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Whether the layer is dynamic (eager-only); set in the constructor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_dynamic</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">())</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">stateful</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_stateful</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">())</span>

  <span class="nd">@stateful</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">stateful</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span>

  <span class="nd">@trainable</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">():</span>
      <span class="n">layer</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">activity_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optional regularizer function for the output of this layer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span>

  <span class="nd">@activity_regularizer</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">activity_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optional regularizer function for the output of this layer.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span> <span class="o">=</span> <span class="n">regularizer</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;`InputSpec` instance(s) describing the input format for this layer.</span>

<span class="sd">    When you create a layer subclass, you can set `self.input_spec` to enable</span>
<span class="sd">    the layer to run input compatibility checks when it is called.</span>
<span class="sd">    Consider a `Conv2D` layer: it can only be called on a single input tensor</span>
<span class="sd">    of rank 4. As such, you can set, in `__init__()`:</span>

<span class="sd">    ```python</span>
<span class="sd">    self.input_spec = tf.keras.layers.InputSpec(ndim=4)</span>
<span class="sd">    ```</span>

<span class="sd">    Now, if you try to call the layer on an input that isn&#39;t rank 4</span>
<span class="sd">    (for instance, an input of shape `(2,)`, it will raise a nicely-formatted</span>
<span class="sd">    error:</span>

<span class="sd">    ```</span>
<span class="sd">    ValueError: Input 0 of layer conv2d is incompatible with the layer:</span>
<span class="sd">    expected ndim=4, found ndim=1. Full shape received: [2]</span>
<span class="sd">    ```</span>

<span class="sd">    Input checks that can be specified via `input_spec` include:</span>
<span class="sd">    - Structure (e.g. a single input, a list of 2 inputs, etc)</span>
<span class="sd">    - Shape</span>
<span class="sd">    - Rank (ndim)</span>
<span class="sd">    - Dtype</span>

<span class="sd">    For more information, see `tf.keras.layers.InputSpec`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `tf.keras.layers.InputSpec` instance, or nested structure thereof.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_spec</span>

  <span class="nd">@input_spec</span><span class="o">.</span><span class="n">setter</span>
  <span class="c1"># Must be decorated to prevent tracking, since the input_spec can be nested</span>
  <span class="c1"># InputSpec objects.</span>
  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">input_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">InputSpec</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer input_spec must be an instance of InputSpec. &#39;</span>
                        <span class="s1">&#39;Got: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_spec</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List of all trainable weights tracked by this layer.</span>

<span class="sd">    Trainable weights are updated via gradient descent during training.</span>

<span class="sd">    Note: This will not track the weights of nested `tf.Modules` that are not</span>
<span class="sd">    themselves Keras layers.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of trainable variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
      <span class="n">children_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;trainable_weights&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dedup_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">+</span> <span class="n">children_weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">non_trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List of all non-trainable weights tracked by this layer.</span>

<span class="sd">    Non-trainable weights are *not* updated during training. They are expected</span>
<span class="sd">    to be updated manually in `call()`.</span>

<span class="sd">    Note: This will not track the weights of nested `tf.Modules` that are not</span>
<span class="sd">    themselves Keras layers.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of non-trainable variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
      <span class="n">children_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span>
          <span class="s1">&#39;non_trainable_weights&#39;</span><span class="p">)</span>
      <span class="n">non_trainable_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="o">+</span> <span class="n">children_weights</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">children_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">)</span>
      <span class="n">non_trainable_weights</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="o">+</span>
          <span class="n">children_weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dedup_weights</span><span class="p">(</span><span class="n">non_trainable_weights</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of all layer variables/weights.</span>

<span class="sd">    Note: This will not track the weights of nested `tf.Modules` that are not</span>
<span class="sd">    themselves Keras layers.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_weights</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>
  <span class="k">def</span> <span class="nf">updates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.updates` will be removed in a future version. &#39;</span>
                  <span class="s1">&#39;This property should not be used in TensorFlow 2.0, &#39;</span>
                  <span class="s1">&#39;as `updates` are applied automatically.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensors_enabled</span><span class="p">():</span>
      <span class="k">return</span> <span class="p">[]</span>

    <span class="n">collected_updates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">all_layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">layer</span><span class="o">.</span><span class="n">stateful</span><span class="p">:</span>
          <span class="k">continue</span>
        <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">_updates</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">()</span>
          <span class="n">collected_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">collected_updates</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List of losses added using the `add_loss()` API.</span>

<span class="sd">    Variable regularization tensors are created when this property is accessed,</span>
<span class="sd">    so it is eager safe: accessing `losses` under a `tf.GradientTape` will</span>
<span class="sd">    propagate gradients back to the corresponding variables.</span>

<span class="sd">    Examples:</span>

<span class="sd">    &gt;&gt;&gt; class MyLayer(tf.keras.layers.Layer):</span>
<span class="sd">    ...   def call(self, inputs):</span>
<span class="sd">    ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>
<span class="sd">    ...     return inputs</span>
<span class="sd">    &gt;&gt;&gt; l = MyLayer()</span>
<span class="sd">    &gt;&gt;&gt; l(np.ones((10, 1)))</span>
<span class="sd">    &gt;&gt;&gt; l.losses</span>
<span class="sd">    [1.0]</span>

<span class="sd">    &gt;&gt;&gt; inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    &gt;&gt;&gt; x = tf.keras.layers.Dense(10)(inputs)</span>
<span class="sd">    &gt;&gt;&gt; outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    &gt;&gt;&gt; model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    &gt;&gt;&gt; # Activity regularization.</span>
<span class="sd">    &gt;&gt;&gt; len(model.losses)</span>
<span class="sd">    0</span>
<span class="sd">    &gt;&gt;&gt; model.add_loss(tf.abs(tf.reduce_mean(x)))</span>
<span class="sd">    &gt;&gt;&gt; len(model.losses)</span>
<span class="sd">    1</span>

<span class="sd">    &gt;&gt;&gt; inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    &gt;&gt;&gt; d = tf.keras.layers.Dense(10, kernel_initializer=&#39;ones&#39;)</span>
<span class="sd">    &gt;&gt;&gt; x = d(inputs)</span>
<span class="sd">    &gt;&gt;&gt; outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    &gt;&gt;&gt; model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    &gt;&gt;&gt; # Weight regularization.</span>
<span class="sd">    &gt;&gt;&gt; model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>
<span class="sd">    &gt;&gt;&gt; model.losses</span>
<span class="sd">    [&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;]</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">collected_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">():</span>
      <span class="c1"># If any eager losses are present, we assume the model to be part of an</span>
      <span class="c1"># eager training loop (either a custom one or the one used when</span>
      <span class="c1"># `run_eagerly=True`) and so we always return just the eager losses.</span>
      <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">_eager_losses</span><span class="p">:</span>
        <span class="c1"># Filter placeholder losses that may have been added by revived layers.</span>
        <span class="c1"># (see base_layer_utils for details).</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_eager_losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span>
            <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">REVIVED_LOSS_PLACEHOLDER</span><span class="p">):</span>
          <span class="n">collected_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_eager_losses</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">collected_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_losses</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">regularizer</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">_callable_losses</span><span class="p">:</span>
        <span class="n">loss_tensor</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">loss_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">collected_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">collected_losses</span>

  <span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="sd">    Some losses (for instance, activity regularization losses) may be dependent</span>
<span class="sd">    on the inputs passed when calling a layer. Hence, when reusing the same</span>
<span class="sd">    layer on different inputs `a` and `b`, some entries in `layer.losses` may</span>
<span class="sd">    be dependent on `a` and some on `b`. This method automatically keeps track</span>
<span class="sd">    of dependencies.</span>

<span class="sd">    This method can be used inside a subclassed layer or model&#39;s `call`</span>
<span class="sd">    function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    class MyLayer(tf.keras.layers.Layer):</span>
<span class="sd">      def call(self, inputs):</span>
<span class="sd">        self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>
<span class="sd">        return inputs</span>
<span class="sd">    ```</span>

<span class="sd">    This method can also be called directly on a Functional Model during</span>
<span class="sd">    construction. In this case, any loss Tensors passed to this Model must</span>
<span class="sd">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>
<span class="sd">    losses become part of the model&#39;s topology and are tracked in `get_config`.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    x = tf.keras.layers.Dense(10)(inputs)</span>
<span class="sd">    outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    # Activity regularization.</span>
<span class="sd">    model.add_loss(tf.abs(tf.reduce_mean(x)))</span>
<span class="sd">    ```</span>

<span class="sd">    If this is not the case for your loss (if, for example, your loss references</span>
<span class="sd">    a `Variable` of one of the model&#39;s layers), you can wrap your loss in a</span>
<span class="sd">    zero-argument lambda. These losses are not tracked as part of the model&#39;s</span>
<span class="sd">    topology since they can&#39;t be serialized.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    d = tf.keras.layers.Dense(10)</span>
<span class="sd">    x = d(inputs)</span>
<span class="sd">    outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    # Weight regularization.</span>
<span class="sd">    model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>
<span class="sd">    ```</span>

<span class="sd">    Arguments:</span>
<span class="sd">      losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses</span>
<span class="sd">        may also be zero-argument callables which create a loss tensor.</span>
<span class="sd">      **kwargs: Additional keyword arguments for backward compatibility.</span>
<span class="sd">        Accepted values:</span>
<span class="sd">          inputs - Deprecated, will be automatically inferred.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;inputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword arguments: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">(),))</span>

    <span class="k">def</span> <span class="nf">_tag_callable</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Tags callable loss tensor as `_unconditional_loss`.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="c1"># We run the loss without autocasting, as regularizers are often</span>
        <span class="c1"># numerically unstable in float16.</span>
        <span class="k">with</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">enable_auto_cast_variables</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Will be filtered out when computing the .losses property</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_v2_with_dispatch</span><span class="p">(</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">floatx</span><span class="p">())</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">_unconditional_loss</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">loss</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

    <span class="n">callable_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">symbolic_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">callable_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_tag_callable</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
          <span class="n">loss</span><span class="p">,</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_v2_with_dispatch</span><span class="p">(</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">floatx</span><span class="p">())</span>
      <span class="c1"># TF Functions should take the eager path.</span>
      <span class="k">if</span> <span class="p">((</span><span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">or</span>
           <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">))</span> <span class="ow">and</span>
          <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_tf_function</span><span class="p">()):</span>
        <span class="n">symbolic_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">eager_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">callable_losses</span><span class="p">)</span>

    <span class="n">in_call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">call_context</span><span class="p">()</span><span class="o">.</span><span class="n">in_call</span>
    <span class="k">if</span> <span class="n">eager_losses</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">in_call_context</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Expected a symbolic Tensors or a callable for the loss value. &#39;</span>
          <span class="s1">&#39;Please wrap your loss computation in a zero argument `lambda`.&#39;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">eager_losses</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">in_call_context</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensors_enabled</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="ow">in</span> <span class="n">symbolic_losses</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="ow">in</span> <span class="n">symbolic_losses</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph_network_add_loss</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_clear_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Used every step in eager to reset losses.&quot;&quot;&quot;</span>
    <span class="c1"># Set to thread local directly to avoid Layer.__setattr__ overhead.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>  <span class="c1"># Fast path for single Layer.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">():</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List of metrics added using the `add_metric()` API.</span>

<span class="sd">    Example:</span>

<span class="sd">    &gt;&gt;&gt; input = tf.keras.layers.Input(shape=(3,))</span>
<span class="sd">    &gt;&gt;&gt; d = tf.keras.layers.Dense(2)</span>
<span class="sd">    &gt;&gt;&gt; output = d(input)</span>
<span class="sd">    &gt;&gt;&gt; d.add_metric(tf.reduce_max(output), name=&#39;max&#39;)</span>
<span class="sd">    &gt;&gt;&gt; d.add_metric(tf.reduce_min(output), name=&#39;min&#39;)</span>
<span class="sd">    &gt;&gt;&gt; [m.name for m in d.metrics]</span>
<span class="sd">    [&#39;max&#39;, &#39;min&#39;]</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Metric` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">collected_metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">():</span>
      <span class="k">with</span> <span class="n">layer</span><span class="o">.</span><span class="n">_metrics_lock</span><span class="p">:</span>
        <span class="n">collected_metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_metrics</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">collected_metrics</span>

  <span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds metric tensor to the layer.</span>

<span class="sd">    This method can be used inside the `call()` method of a subclassed layer</span>
<span class="sd">    or model.</span>

<span class="sd">    ```python</span>
<span class="sd">    class MyMetricLayer(tf.keras.layers.Layer):</span>
<span class="sd">      def __init__(self):</span>
<span class="sd">        super(MyMetricLayer, self).__init__(name=&#39;my_metric_layer&#39;)</span>
<span class="sd">        self.mean = tf.keras.metrics.Mean(name=&#39;metric_1&#39;)</span>

<span class="sd">      def call(self, inputs):</span>
<span class="sd">        self.add_metric(self.mean(x))</span>
<span class="sd">        self.add_metric(tf.reduce_sum(x), name=&#39;metric_2&#39;)</span>
<span class="sd">        return inputs</span>
<span class="sd">    ```</span>

<span class="sd">    This method can also be called directly on a Functional Model during</span>
<span class="sd">    construction. In this case, any tensor passed to this Model must</span>
<span class="sd">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>
<span class="sd">    metrics become part of the model&#39;s topology and are tracked when you</span>
<span class="sd">    save the model via `save()`.</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    x = tf.keras.layers.Dense(10)(inputs)</span>
<span class="sd">    outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    model.add_metric(math_ops.reduce_sum(x), name=&#39;metric_1&#39;)</span>
<span class="sd">    ```</span>

<span class="sd">    Note: Calling `add_metric()` with the result of a metric object on a</span>
<span class="sd">    Functional Model, as shown in the example below, is not supported. This is</span>
<span class="sd">    because we cannot trace the metric result tensor back to the model&#39;s inputs.</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    x = tf.keras.layers.Dense(10)(inputs)</span>
<span class="sd">    outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    model.add_metric(tf.keras.metrics.Mean()(x), name=&#39;metric_1&#39;)</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      value: Metric tensor.</span>
<span class="sd">      name: String metric name.</span>
<span class="sd">      **kwargs: Additional keyword arguments for backward compatibility.</span>
<span class="sd">        Accepted values:</span>
<span class="sd">        `aggregation` - When the `value` tensor provided is not the result of</span>
<span class="sd">        calling a `keras.Metric` instance, it will be aggregated by default</span>
<span class="sd">        using a `keras.Metric.Mean`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span>
        <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">kwargs_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;aggregation&#39;</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword arguments: &#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>

    <span class="n">from_metric_obj</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_metric_obj&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensors_enabled</span><span class="p">():</span>
      <span class="n">is_symbolic</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">is_symbolic</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">in_call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">call_context</span><span class="p">()</span><span class="o">.</span><span class="n">in_call</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">from_metric_obj</span><span class="p">:</span>
      <span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x))`</span>
      <span class="c1"># In eager mode, we use metric name to lookup a metric. Without a name,</span>
      <span class="c1"># a new Mean metric wrapper will be created on every model/layer call.</span>
      <span class="c1"># So, we raise an error when no name is provided.</span>
      <span class="c1"># We will do the same for symbolic mode for consistency although a name</span>
      <span class="c1"># will be generated if no name is provided.</span>

      <span class="c1"># We will not raise this error in the foll use case for the sake of</span>
      <span class="c1"># consistency as name in provided in the metric constructor.</span>
      <span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span>
      <span class="c1"># model.add_metric(mean(outputs))</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please provide a name for your metric like &#39;</span>
                       <span class="s1">&#39;`self.add_metric(tf.reduce_sum(inputs), &#39;</span>
                       <span class="s1">&#39;name=</span><span class="se">\&#39;</span><span class="s1">mean_activation</span><span class="se">\&#39;</span><span class="s1">)`&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">from_metric_obj</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">_metric_obj</span><span class="o">.</span><span class="n">name</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">in_call_context</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_symbolic</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected a symbolic Tensor for the metric value, &#39;</span>
                       <span class="s1">&#39;received: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

    <span class="c1"># If a metric was added in a Layer&#39;s `call` or `build`.</span>
    <span class="k">if</span> <span class="n">in_call_context</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
      <span class="c1"># TF Function path should take the eager path.</span>

      <span class="c1"># If the given metric is available in `metrics` list we just update state</span>
      <span class="c1"># on it, otherwise we create a new metric instance and</span>
      <span class="c1"># add it to the `metrics` list.</span>
      <span class="n">metric_obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_metric_obj&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="c1"># Tensors that come from a Metric object already updated the Metric state.</span>
      <span class="n">should_update_state</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">metric_obj</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">metric_obj</span><span class="o">.</span><span class="n">name</span> <span class="k">if</span> <span class="n">metric_obj</span> <span class="k">else</span> <span class="n">name</span>

      <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics_lock</span><span class="p">:</span>
        <span class="n">match</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
          <span class="n">metric_obj</span> <span class="o">=</span> <span class="n">match</span>
        <span class="k">elif</span> <span class="n">metric_obj</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Build the metric object with the value&#39;s dtype if it defines one</span>
          <span class="n">metric_obj</span> <span class="o">=</span> <span class="n">metrics_mod</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">should_update_state</span><span class="p">:</span>
        <span class="n">metric_obj</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">from_metric_obj</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Using the result of calling a `Metric` object &#39;</span>
                         <span class="s1">&#39;when calling `add_metric` on a Functional &#39;</span>
                         <span class="s1">&#39;Model is not supported. Please pass the &#39;</span>
                         <span class="s1">&#39;Tensor to monitor directly.&#39;</span><span class="p">)</span>

      <span class="c1"># Insert layers into the Keras Graph Network.</span>
      <span class="n">aggregation</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">from_metric_obj</span> <span class="k">else</span> <span class="s1">&#39;mean&#39;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_network_add_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add update op(s), potentially dependent on layer inputs.</span>

<span class="sd">    Weight updates (for instance, the updates of the moving mean and variance</span>
<span class="sd">    in a BatchNormalization layer) may be dependent on the inputs passed</span>
<span class="sd">    when calling a layer. Hence, when reusing the same layer on</span>
<span class="sd">    different inputs `a` and `b`, some entries in `layer.updates` may be</span>
<span class="sd">    dependent on `a` and some on `b`. This method automatically keeps track</span>
<span class="sd">    of dependencies.</span>

<span class="sd">    This call is ignored when eager execution is enabled (in that case, variable</span>
<span class="sd">    updates are run on the fly and thus do not need to be tracked for later</span>
<span class="sd">    execution).</span>

<span class="sd">    Arguments:</span>
<span class="sd">      updates: Update op, or list/tuple of update ops, or zero-arg callable</span>
<span class="sd">        that returns an update op. A zero-arg callable should be passed in</span>
<span class="sd">        order to disable running the updates by setting `trainable=False`</span>
<span class="sd">        on this Layer, when executing in Eager mode.</span>
<span class="sd">      inputs: Deprecated, will be automatically inferred.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">tf_logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s1">&#39;`add_update` `inputs` kwarg has been deprecated. You no longer need &#39;</span>
          <span class="s1">&#39;to pass a value to `inputs` as it is being automatically inferred.&#39;</span><span class="p">)</span>
    <span class="n">call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">call_context</span><span class="p">()</span>
    <span class="c1"># No need to run updates during Functional API construction.</span>
    <span class="k">if</span> <span class="n">call_context</span><span class="o">.</span><span class="n">in_keras_graph</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="c1"># Callable updates are disabled by setting `trainable=False`.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">call_context</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">update</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">updates</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">update</span><span class="p">):</span>
          <span class="n">update</span><span class="p">()</span>  <span class="c1"># pylint: disable=not-callable</span>

  <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the weights of the layer, from Numpy arrays.</span>

<span class="sd">    The weights of a layer represent the state of the layer. This function</span>
<span class="sd">    sets the weight values from numpy arrays. The weight values should be</span>
<span class="sd">    passed in the order they are created by the layer. Note that the layer&#39;s</span>
<span class="sd">    weights must be instantiated before calling this function by calling</span>
<span class="sd">    the layer.</span>

<span class="sd">    For example, a Dense layer returns a list of two values-- per-output</span>
<span class="sd">    weights and the bias value. These can be used to set the weights of another</span>
<span class="sd">    Dense layer:</span>

<span class="sd">    &gt;&gt;&gt; a = tf.keras.layers.Dense(1,</span>
<span class="sd">    ...   kernel_initializer=tf.constant_initializer(1.))</span>
<span class="sd">    &gt;&gt;&gt; a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))</span>
<span class="sd">    &gt;&gt;&gt; a.get_weights()</span>
<span class="sd">    [array([[1.],</span>
<span class="sd">           [1.],</span>
<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>
<span class="sd">    &gt;&gt;&gt; b = tf.keras.layers.Dense(1,</span>
<span class="sd">    ...   kernel_initializer=tf.constant_initializer(2.))</span>
<span class="sd">    &gt;&gt;&gt; b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))</span>
<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>
<span class="sd">    [array([[2.],</span>
<span class="sd">           [2.],</span>
<span class="sd">           [2.]], dtype=float32), array([0.], dtype=float32)]</span>
<span class="sd">    &gt;&gt;&gt; b.set_weights(a.get_weights())</span>
<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>
<span class="sd">    [array([[1.],</span>
<span class="sd">           [1.],</span>
<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    Arguments:</span>
<span class="sd">        weights: a list of Numpy arrays. The number</span>
<span class="sd">            of arrays and their shape must match</span>
<span class="sd">            number of the dimensions of the weights</span>
<span class="sd">            of the layer (i.e. it should match the</span>
<span class="sd">            output of `get_weights`).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the provided weights list does not match the</span>
<span class="sd">            layer&#39;s specifications.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

    <span class="n">expected_num_weights</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span>
        <span class="n">expected_num_weights</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">expected_num_weights</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">expected_num_weights</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;You called `set_weights(weights)` on layer &quot;</span><span class="si">%s</span><span class="s1">&quot; &#39;</span>
          <span class="s1">&#39;with a weight list of length </span><span class="si">%s</span><span class="s1">, but the layer was &#39;</span>
          <span class="s1">&#39;expecting </span><span class="si">%s</span><span class="s1"> weights. Provided weights: </span><span class="si">%s</span><span class="s1">...&#39;</span> <span class="o">%</span>
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">expected_num_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]))</span>

    <span class="n">weight_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">weight_value_tuples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span>
        <span class="n">num_tensors</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">:</span><span class="n">weight_index</span> <span class="o">+</span> <span class="n">num_tensors</span><span class="p">]</span>
        <span class="n">param</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
        <span class="n">weight_index</span> <span class="o">+=</span> <span class="n">num_tensors</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">]</span>
        <span class="n">ref_shape</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ref_shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s1">&#39;Layer weight shape </span><span class="si">%s</span><span class="s1"> not compatible with provided weight &#39;</span>
              <span class="s1">&#39;shape </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">ref_shape</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">weight_value_tuples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">param</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>
        <span class="n">weight_index</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">backend</span><span class="o">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current weights of the layer.</span>

<span class="sd">    The weights of a layer represent the state of the layer. This function</span>
<span class="sd">    returns both trainable and non-trainable weight values associated with this</span>
<span class="sd">    layer as a list of Numpy arrays, which can in turn be used to load state</span>
<span class="sd">    into similarly parameterized layers.</span>

<span class="sd">    For example, a Dense layer returns a list of two values-- per-output</span>
<span class="sd">    weights and the bias value. These can be used to set the weights of another</span>
<span class="sd">    Dense layer:</span>

<span class="sd">    &gt;&gt;&gt; a = tf.keras.layers.Dense(1,</span>
<span class="sd">    ...   kernel_initializer=tf.constant_initializer(1.))</span>
<span class="sd">    &gt;&gt;&gt; a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))</span>
<span class="sd">    &gt;&gt;&gt; a.get_weights()</span>
<span class="sd">    [array([[1.],</span>
<span class="sd">           [1.],</span>
<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>
<span class="sd">    &gt;&gt;&gt; b = tf.keras.layers.Dense(1,</span>
<span class="sd">    ...   kernel_initializer=tf.constant_initializer(2.))</span>
<span class="sd">    &gt;&gt;&gt; b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))</span>
<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>
<span class="sd">    [array([[2.],</span>
<span class="sd">           [2.],</span>
<span class="sd">           [2.]], dtype=float32), array([0.], dtype=float32)]</span>
<span class="sd">    &gt;&gt;&gt; b.set_weights(a.get_weights())</span>
<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>
<span class="sd">    [array([[1.],</span>
<span class="sd">           [1.],</span>
<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    Returns:</span>
<span class="sd">        Weights values as a list of numpy arrays.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
    <span class="n">output_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span>
        <span class="n">output_weights</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">get_tensors</span><span class="p">())</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">output_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">output_weights</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>
  <span class="k">def</span> <span class="nf">get_updates_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use!</span>

<span class="sd">    Retrieves updates relevant to a specific set of inputs.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of update ops of the layer that depend on `inputs`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_updates_for` is deprecated and &#39;</span>
                  <span class="s1">&#39;will be removed in a future version. &#39;</span>
                  <span class="s1">&#39;Please use `layer.updates` method instead.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">updates</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>
  <span class="k">def</span> <span class="nf">get_losses_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use!</span>

<span class="sd">    Retrieves losses relevant to a specific set of inputs.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of loss tensors of the layer that depend on `inputs`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_losses_for` is deprecated and &#39;</span>
                  <span class="s1">&#39;will be removed in a future version. &#39;</span>
                  <span class="s1">&#39;Please use `layer.losses` instead.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A mask tensor</span>
<span class="sd">        (or list of tensors if the layer has multiple inputs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A mask tensor</span>
<span class="sd">        (or list of tensors if the layer has multiple outputs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">input_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one inbound node,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Input mask tensor (potentially None) or list of input</span>
<span class="sd">        mask tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer is connected to</span>
<span class="sd">        more than one incoming layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">output_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one inbound node,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output mask tensor (potentially None) or list of output</span>
<span class="sd">        mask tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer is connected to</span>
<span class="sd">        more than one incoming layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A shape tuple</span>
<span class="sd">        (or list of shape tuples if the layer has multiple inputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;input_shapes&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;input shape&#39;</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A shape tuple</span>
<span class="sd">        (or list of shape tuples if the layer has multiple outputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;output_shapes&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;output shape&#39;</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;input&#39;</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;output&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one input,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Input tensor or list of input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">      AttributeError: If no inbound nodes are found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                           <span class="s1">&#39; is not connected, no input to return.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one output,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor or list of output tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      AttributeError: if the layer is connected to more than one incoming</span>
<span class="sd">        layers.</span>
<span class="sd">      RuntimeError: if called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39; has no inbound nodes.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span> <span class="s1">&#39;output&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input shape(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one input,</span>
<span class="sd">    i.e. if it is connected to one incoming layer, or if all inputs</span>
<span class="sd">    have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Input shape, as an integer shape tuple</span>
<span class="sd">        (or list of shape tuples, one tuple per input tensor).</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer has no defined input_shape.</span>
<span class="sd">        RuntimeError: if called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer has never been called &#39;</span>
                           <span class="s1">&#39;and thus has no defined input shape.&#39;</span><span class="p">)</span>
    <span class="n">all_input_shapes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">input_shapes</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_input_shapes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">input_shapes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer &quot;&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span>
                           <span class="s1">&#39; has multiple inbound nodes, &#39;</span>
                           <span class="s1">&#39;with different input shapes. Hence &#39;</span>
                           <span class="s1">&#39;the notion of &quot;input shape&quot; is &#39;</span>
                           <span class="s1">&#39;ill-defined for the layer. &#39;</span>
                           <span class="s1">&#39;Use `get_input_shape_at(node_index)` &#39;</span>
                           <span class="s1">&#39;instead.&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Count the total number of scalars composing the weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An integer count.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if the layer isn&#39;t yet built</span>
<span class="sd">          (in which case its weights aren&#39;t yet defined).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You tried to call `count_params` on &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                         <span class="s1">&#39;, but the layer isn</span><span class="se">\&#39;</span><span class="s1">t built. &#39;</span>
                         <span class="s1">&#39;You can build it manually via: `&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                         <span class="s1">&#39;.build(batch_input_shape)`.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_utils</span><span class="o">.</span><span class="n">count_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output shape(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has one output,</span>
<span class="sd">    or if all outputs have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output shape, as an integer shape tuple</span>
<span class="sd">        (or list of shape tuples, one tuple per output tensor).</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer has no defined output shape.</span>
<span class="sd">        RuntimeError: if called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer has never been called &#39;</span>
                           <span class="s1">&#39;and thus has no defined output shape.&#39;</span><span class="p">)</span>
    <span class="n">all_output_shapes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">output_shapes</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_output_shapes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output_shapes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer &quot;</span><span class="si">%s</span><span class="s1">&quot;&#39;</span>
                           <span class="s1">&#39; has multiple inbound nodes, &#39;</span>
                           <span class="s1">&#39;with different output shapes. Hence &#39;</span>
                           <span class="s1">&#39;the notion of &quot;output shape&quot; is &#39;</span>
                           <span class="s1">&#39;ill-defined for the layer. &#39;</span>
                           <span class="s1">&#39;Use `get_output_shape_at(node_index)` &#39;</span>
                           <span class="s1">&#39;instead.&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">inbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use! Only for compatibility with external Keras.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">outbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use! Only for compatibility with external Keras.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes</span>

  <span class="c1">##############################################################################</span>
  <span class="c1"># Methods &amp; attributes below are public aliases of other methods.            #</span>
  <span class="c1">##############################################################################</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use!</span>

<span class="sd">    This is an alias of `self.__call__`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: Input tensor(s).</span>
<span class="sd">      *args: additional positional arguments to be passed to `self.call`.</span>
<span class="sd">      **kwargs: additional keyword arguments to be passed to `self.call`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.apply` is deprecated and &#39;</span>
                  <span class="s1">&#39;will be removed in a future version. &#39;</span>
                  <span class="s1">&#39;Please use `layer.__call__` method instead.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use! Alias for `add_weight`.&quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.add_variable` is deprecated and &#39;</span>
                  <span class="s1">&#39;will be removed in a future version. &#39;</span>
                  <span class="s1">&#39;Please use `layer.add_weight` method instead.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>
  <span class="k">def</span> <span class="nf">variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of all layer variables/weights.</span>

<span class="sd">    Alias of `self.weights`.</span>

<span class="sd">    Note: This will not track the weights of nested `tf.Modules` that are not</span>
<span class="sd">    themselves Keras layers.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>
  <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>
  <span class="k">def</span> <span class="nf">non_trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_weights</span>

  <span class="c1">##############################################################################</span>
  <span class="c1"># Methods &amp; attributes below are all private and only used by the framework. #</span>
  <span class="c1">##############################################################################</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_inbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes_value</span>

  <span class="nd">@_inbound_nodes</span><span class="o">.</span><span class="n">setter</span>
  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_inbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes_value</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_outbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes_value</span>

  <span class="nd">@_outbound_nodes</span><span class="o">.</span><span class="n">setter</span>
  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_outbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes_value</span> <span class="o">=</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">_set_dtype_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets self._dtype_policy.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dtype</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">global_policy</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;mixed_float16&#39;</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">loss_scale_optimizer</span><span class="o">.</span><span class="n">strategy_supports_loss_scaling</span><span class="p">()):</span>
      <span class="c1"># Although only loss scaling doesn&#39;t support certain strategies, to avoid</span>
      <span class="c1"># confusion, we disallow the &#39;mixed_float16&#39; policy with unsupported</span>
      <span class="c1"># strategies. This is because &#39;mixed_float16&#39; requires loss scaling for</span>
      <span class="c1"># numeric stability.</span>
      <span class="n">strategy</span> <span class="o">=</span> <span class="n">ds_context</span><span class="o">.</span><span class="n">get_strategy</span><span class="p">()</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Mixed precision is not supported with the &#39;</span>
                       <span class="s1">&#39;tf.distribute.Strategy: </span><span class="si">%s</span><span class="s1">. Either stop using mixed &#39;</span>
                       <span class="s1">&#39;precision by removing the use of the &quot;</span><span class="si">%s</span><span class="s1">&quot; policy or &#39;</span>
                       <span class="s1">&#39;use a different Strategy, e.g. a MirroredStrategy.&#39;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="c1"># Performance optimization: cache the compute dtype as a Dtype object or</span>
    <span class="c1"># None, so that str to Dtype conversion doesn&#39;t happen in Layer.__call__.</span>
    <span class="c1"># TODO(b/157486353): Investigate returning DTypes in Policy.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The dtype policy associated with this layer.</span>

<span class="sd">    This is an instance of a `tf.keras.mixed_precision.Policy`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">compute_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The dtype of the layer&#39;s computations.</span>

<span class="sd">    This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless</span>
<span class="sd">    mixed precision is used, this is the same as `Layer.dtype`, the dtype of</span>
<span class="sd">    the weights.</span>

<span class="sd">    Layers automatically cast their inputs to the compute dtype, which causes</span>
<span class="sd">    computations and the output to be in the compute dtype as well. This is done</span>
<span class="sd">    by the base Layer class in `Layer.__call__`, so you do not have to insert</span>
<span class="sd">    these casts if implementing your own layer.</span>

<span class="sd">    Layers often perform certain internal computations in higher precision when</span>
<span class="sd">    `compute_dtype` is float16 or bfloat16 for numeric stability. The output</span>
<span class="sd">    will still typically be float16 or bfloat16 in such cases.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The layer&#39;s compute dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">compute_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_compute_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated alias of `compute_dtype`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">compute_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">variable_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias of `Layer.dtype`, the dtype of the weights.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>

  <span class="k">def</span> <span class="nf">_maybe_cast_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_list</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Maybe casts the inputs to the compute dtype.</span>

<span class="sd">    If self._compute_dtype is floating-point, and self_autocast is True,</span>
<span class="sd">    floating-point inputs are casted to self._compute_dtype.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: Input tensor, or structure of input tensors.</span>
<span class="sd">      input_list: Flat list of input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `inputs`, but tensors may have been casted to self._compute_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">input_list</span><span class="p">:</span>
      <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">compute_dtype_object</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span>
    <span class="n">should_autocast</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_autocast</span> <span class="ow">and</span> <span class="n">compute_dtype_object</span> <span class="ow">and</span>
        <span class="n">compute_dtype_object</span><span class="o">.</span><span class="n">is_floating</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">should_autocast</span> <span class="ow">and</span>
        <span class="nb">any</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_should_cast_single_input</span><span class="p">,</span> <span class="n">input_list</span><span class="p">))):</span>
      <span class="c1"># Only perform expensive `nest` operation when needed.</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cast_single_input</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">inputs</span>

  <span class="k">def</span> <span class="nf">_should_cast_single_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_AUTOCAST_TYPES</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span> <span class="ow">and</span>
              <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">_cast_single_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Cast a single Tensor or TensorSpec to the compute dtype.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_cast_single_input</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype_object</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>

  <span class="c1"># _dtype used to be an attribute set in the constructor. We still expose it</span>
  <span class="c1"># because some clients still use it.</span>
  <span class="c1"># TODO(reedwm): Deprecate, then remove the _dtype property.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># This is equivalent to returning self.dtype . We do not return self.dtype</span>
    <span class="c1"># as it would cause infinite recursion in a few subclasses, which override</span>
    <span class="c1"># &quot;dtype&quot; to return self._dtype.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">variable_dtype</span>

  <span class="nd">@_dtype</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tf2</span><span class="o">.</span><span class="n">enabled</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
    <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
    <span class="n">current_name_scope</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">current_name_scope</span><span class="p">:</span>
      <span class="n">name_scope</span> <span class="o">=</span> <span class="n">current_name_scope</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span> <span class="o">+</span> <span class="n">name_scope</span>
    <span class="k">if</span> <span class="n">name_scope</span><span class="p">:</span>
      <span class="c1"># Note that the trailing `/` prevents autogenerated</span>
      <span class="c1"># numerical suffixes to get appended. It will also fully reset</span>
      <span class="c1"># nested name scope (i.e. the outer name scope has no effect).</span>
      <span class="n">name_scope</span> <span class="o">+=</span> <span class="s1">&#39;/&#39;</span>
    <span class="k">return</span> <span class="n">name_scope</span>

  <span class="k">def</span> <span class="nf">_init_set_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">zero_based</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">unique_object_name</span><span class="p">(</span>
          <span class="n">generic_utils</span><span class="o">.</span><span class="n">to_snake_case</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span>
          <span class="n">zero_based</span><span class="o">=</span><span class="n">zero_based</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">backend</span><span class="o">.</span><span class="n">observe_object_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>

  <span class="k">def</span> <span class="nf">_get_existing_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">match</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">name</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">match</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Please provide different names for the metrics you have added. &#39;</span>
          <span class="s1">&#39;We found </span><span class="si">{}</span><span class="s1"> metrics with the name: &quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">match</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">match</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_handle_weight_regularization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create lambdas which compute regularization losses.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_loss_for_variable</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Creates a regularization loss `Tensor` for variable `v`.&quot;&quot;&quot;</span>
      <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/Regularizer&#39;</span><span class="p">):</span>
        <span class="n">regularization</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">regularization</span>

    <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_loss_for_variable</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_loss_for_variable</span><span class="p">,</span> <span class="n">variable</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_handle_activity_regularization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># Apply activity regularization.</span>
    <span class="c1"># Note that it should be applied every time the layer creates a new</span>
    <span class="c1"># output, since it is output-specific.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span><span class="p">:</span>
      <span class="n">output_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;ActivityRegularizer&#39;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">output_list</span><span class="p">:</span>
          <span class="n">activity_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
              <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">activity_loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
          <span class="c1"># Make activity regularization strength batch-agnostic.</span>
          <span class="n">mean_activity_loss</span> <span class="o">=</span> <span class="n">activity_loss</span> <span class="o">/</span> <span class="n">batch_size</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">mean_activity_loss</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_mask_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">,</span> <span class="n">build_graph</span><span class="p">):</span>
    <span class="c1"># Many `Layer`s don&#39;t need to call `compute_mask`.</span>
    <span class="c1"># This method is optimized to do as little work as needed for the common</span>
    <span class="c1"># case.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">flat_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="n">mask_already_computed</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_compute_output_and_mask_jointly&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">all</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">mask_already_computed</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">build_graph</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_keras_history_checked</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="n">output_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">flat_masks</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_masks</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">,</span> <span class="n">flat_masks</span><span class="p">):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">tensor</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="n">mask</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="c1"># C Type such as np.ndarray.</span>
        <span class="k">pass</span>

    <span class="k">if</span> <span class="n">build_graph</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_keras_history_checked</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_mask_keras_history_checked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flat_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Do not track masks for `TensorFlowOpLayer` construction.</span>
        <span class="n">output</span><span class="o">.</span><span class="n">_keras_mask</span><span class="o">.</span><span class="n">_keras_history_checked</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_get_input_masks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_mask_arg</span><span class="p">:</span>
      <span class="c1"># Input masks only need to be retrieved if they are needed for `call`</span>
      <span class="c1"># or `compute_mask`.</span>
      <span class="n">input_masks</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">implicit_mask</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_arg_was_passed</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
      <span class="n">input_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_call_arg_value</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
      <span class="n">implicit_mask</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_masks</span> <span class="o">=</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">]</span>
      <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">input_masks</span><span class="p">):</span>
        <span class="n">input_masks</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">implicit_mask</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Only do expensive `nest` op when masking is actually being used.</span>
        <span class="n">input_masks</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_masks</span><span class="p">)</span>
        <span class="n">implicit_mask</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">input_masks</span><span class="p">,</span> <span class="n">implicit_mask</span>

  <span class="k">def</span> <span class="nf">_call_arg_was_passed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">inputs_in_args</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Performance optimization: do no work in most common case.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">True</span>
    <span class="n">call_fn_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs_in_args</span><span class="p">:</span>
      <span class="c1"># Ignore `inputs` arg.</span>
      <span class="n">call_fn_args</span> <span class="o">=</span> <span class="n">call_fn_args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">call_fn_args</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_get_call_arg_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">inputs_in_args</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]</span>
    <span class="n">call_fn_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs_in_args</span><span class="p">:</span>
      <span class="c1"># Ignore `inputs` arg.</span>
      <span class="n">call_fn_args</span> <span class="o">=</span> <span class="n">call_fn_args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">args_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">call_fn_args</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">args_dict</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_set_call_arg_value</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">new_value</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
      <span class="n">kwargs</span><span class="p">,</span> <span class="n">inputs_in_args</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pop_kwarg_if_none</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">arg_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_arg_positions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">arg_pos</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs_in_args</span><span class="p">:</span>
        <span class="c1"># Ignore `inputs` arg.</span>
        <span class="n">arg_pos</span> <span class="o">=</span> <span class="n">arg_pos</span> <span class="o">-</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">arg_pos</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">args</span><span class="p">[</span><span class="n">arg_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_value</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">kwargs</span>
    <span class="k">if</span> <span class="n">new_value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pop_kwarg_if_none</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_value</span>
    <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>

  <span class="k">def</span> <span class="nf">_set_connectivity_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># If the layer returns tensors from its inputs unmodified,</span>
    <span class="c1"># we copy them to avoid loss of KerasHistory metadata.</span>
    <span class="n">flat_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">flat_inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="n">input_ids_set</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flat_inputs</span><span class="p">}</span>
    <span class="n">outputs_copy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="n">input_ids_set</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">outputs_copy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">outputs_copy</span><span class="p">)</span>

    <span class="c1"># Create node, Node wires itself to inbound and outbound layers.</span>
    <span class="c1"># The Node constructor actually updates this layer&#39;s self._inbound_nodes,</span>
    <span class="c1"># sets _keras_history on the outputs, and adds itself to the</span>
    <span class="c1"># `_outbound_nodes` of the layers that produced the inputs to this</span>
    <span class="c1"># layer call.</span>
    <span class="n">node_module</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call_args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">call_kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>

  <span class="k">def</span> <span class="nf">_get_node_attribute_at_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private utility to retrieves an attribute (e.g. inputs) from a node.</span>

<span class="sd">    This is used to implement the methods:</span>
<span class="sd">        - get_input_shape_at</span>
<span class="sd">        - get_output_shape_at</span>
<span class="sd">        - get_input_at</span>
<span class="sd">        etc...</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer index of the node from which</span>
<span class="sd">            to retrieve the attribute.</span>
<span class="sd">        attr: Exact node attribute name.</span>
<span class="sd">        attr_name: Human-readable attribute name, for error messages.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The layer&#39;s attribute `attr` at the node of index `node_index`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If the layer has no inbound nodes, or if called in Eager</span>
<span class="sd">        mode.</span>
<span class="sd">        ValueError: If the index provided does not match any node.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The layer has never been called &#39;</span>
                         <span class="s1">&#39;and thus has no defined &#39;</span> <span class="o">+</span> <span class="n">attr_name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">node_index</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Asked to get &#39;</span> <span class="o">+</span> <span class="n">attr_name</span> <span class="o">+</span> <span class="s1">&#39; at node &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, but the layer has only &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; inbound nodes.&#39;</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_index</span><span class="p">],</span> <span class="n">attr</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">values</span>

  <span class="k">def</span> <span class="nf">_maybe_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="c1"># Check input assumptions set before layer building, e.g. input rank.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="n">input_spec</span><span class="o">.</span><span class="n">assert_input_compatibility</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">input_list</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="o">.</span><span class="n">compute_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="o">.</span><span class="n">name</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
          <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
      <span class="n">input_shapes</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="c1"># Converts Tensors / CompositeTensors to TensorShapes.</span>
      <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">):</span>
        <span class="n">input_shapes</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_shapes</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Converts input shape to TensorShapes.</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">input_shapes</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
          <span class="k">pass</span>
      <span class="c1"># Only call `build` if the user has manually overridden the build method.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">,</span> <span class="s1">&#39;_is_default&#39;</span><span class="p">):</span>
        <span class="c1"># Any setup work performed only once should happen in an `init_scope`</span>
        <span class="c1"># to avoid creating symbolic Tensors that will later pollute any eager</span>
        <span class="c1"># operations.</span>
        <span class="k">with</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">)</span>  <span class="c1"># pylint:disable=not-callable</span>
      <span class="c1"># We must set also ensure that the layer is marked as built, and the build</span>
      <span class="c1"># shape is stored since user defined build functions may not be calling</span>
      <span class="c1"># `super.build()`</span>
      <span class="n">Layer</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">)</span>

    <span class="c1"># Optionally load weight values specified at layer instantiation.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
          <span class="c1"># Using `init_scope` since we want variable assignment in</span>
          <span class="c1"># `set_weights` to be treated like variable initialization.</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_symbolic_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">input_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">output_shapes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">)</span>
    <span class="c1"># Convert to TensorShape so that nest.map_structure will not map into</span>
    <span class="c1"># individual dim of the shape.</span>
    <span class="n">output_shapes</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span><span class="n">output_shapes</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_placeholder_like</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
      <span class="n">ph</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">ph</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">return</span> <span class="n">ph</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_make_placeholder_like</span><span class="p">,</span> <span class="n">output_shapes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_trainable_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the `trainable` state of each sublayer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A dict mapping all sublayers to their `trainable` value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">trainable_state</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakKeyDictionary</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">():</span>
      <span class="n">trainable_state</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span>
    <span class="k">return</span> <span class="n">trainable_state</span>

  <span class="k">def</span> <span class="nf">_set_trainable_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainable_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set `trainable` state for each sublayer.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">trainable_state</span><span class="p">:</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable_state</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_obj_reference_counts</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A dictionary counting the number of attributes referencing an object.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_obj_reference_counts_dict&#39;</span><span class="p">,</span>
                                 <span class="n">object_identity</span><span class="o">.</span><span class="n">ObjectIdentityDictionary</span><span class="p">())</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_reference_counts_dict</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_maybe_create_attribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the attribute with the default value if it hasn&#39;t been created.</span>

<span class="sd">    This is useful for fields that is used for tracking purpose,</span>
<span class="sd">    _trainable_weights, or _layers. Note that user could create a layer subclass</span>
<span class="sd">    and assign an internal field before invoking the Layer.__init__(), the</span>
<span class="sd">    __setattr__() need to create the tracking fields and __init__() need to not</span>
<span class="sd">    override them.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: String, the name of the attribute.</span>
<span class="sd">      default_value: Object, the default value of the attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default_value</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__delattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="c1"># For any super.__delattr__() call, we will directly use the implementation</span>
    <span class="c1"># in Trackable and skip the behavior in AutoTrackable. The Layer was</span>
    <span class="c1"># originally use Trackable as base class, the change of using Module as base</span>
    <span class="c1"># class forced us to have AutoTrackable in the class hierarchy. Skipping</span>
    <span class="c1"># the __delattr__ and __setattr__ in AutoTrackable will keep the status quo.</span>
    <span class="n">existing_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># If this value is replacing an existing object assigned to an attribute, we</span>
    <span class="c1"># should clean it out to avoid leaking memory. First we check if there are</span>
    <span class="c1"># other attributes referencing it.</span>
    <span class="n">reference_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_reference_counts</span>
    <span class="k">if</span> <span class="n">existing_value</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">reference_counts</span><span class="p">:</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="n">reference_count</span> <span class="o">=</span> <span class="n">reference_counts</span><span class="p">[</span><span class="n">existing_value</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">reference_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># There are other remaining references. We can&#39;t remove this object from</span>
      <span class="c1"># _layers etc.</span>
      <span class="n">reference_counts</span><span class="p">[</span><span class="n">existing_value</span><span class="p">]</span> <span class="o">=</span> <span class="n">reference_count</span> <span class="o">-</span> <span class="mi">1</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># This is the last remaining reference.</span>
      <span class="k">del</span> <span class="n">reference_counts</span><span class="p">[</span><span class="n">existing_value</span><span class="p">]</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_value</span><span class="p">,</span> <span class="n">Layer</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">has_weights</span><span class="p">(</span><span class="n">existing_value</span><span class="p">)):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
          <span class="s1">&#39;_layers&#39;</span><span class="p">,</span>
          <span class="p">[</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="k">if</span> <span class="n">l</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">existing_value</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_value</span><span class="p">,</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
          <span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span>
          <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">existing_value</span><span class="p">])</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
          <span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span>
          <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">existing_value</span><span class="p">])</span>

  <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;_self_setattr_tracking&#39;</span> <span class="ow">or</span>
        <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_self_setattr_tracking&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="ow">or</span>
        <span class="c1"># Exclude @property.setters from tracking</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">name</span><span class="p">)):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="p">(</span><span class="s1">&#39;Can</span><span class="se">\&#39;</span><span class="s1">t set the attribute &quot;</span><span class="si">{}</span><span class="s1">&quot;, likely because it conflicts with &#39;</span>
             <span class="s1">&#39;an existing read-only @property of the object. Please choose a &#39;</span>
             <span class="s1">&#39;different name.&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
      <span class="k">return</span>

    <span class="c1"># Keep track of trackable objects, for the needs of `Network.save_weights`.</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">data_structures</span><span class="o">.</span><span class="n">sticky_attribute_assignment</span><span class="p">(</span>
        <span class="n">trackable</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="n">reference_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_reference_counts</span>
    <span class="n">reference_counts</span><span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="n">reference_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># Clean out the old attribute, which clears _layers and _trainable_weights</span>
    <span class="c1"># if necessary.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="c1"># Keep track of metric instance created in subclassed layer.</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">metrics_mod</span><span class="o">.</span><span class="n">Metric</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_metrics&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="c1"># TODO(scottzhu): Need to track Module object as well for weight tracking.</span>
    <span class="c1"># Be careful about metric if it becomes a Module in future.</span>
    <span class="c1"># Append value to self._layers if relevant</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_auto_track_sub_layers&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="ow">and</span>
        <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Layer</span><span class="p">)</span> <span class="ow">or</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">has_weights</span><span class="p">(</span><span class="n">value</span><span class="p">))):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="p">[])</span>
      <span class="c1"># We need to check object identity to avoid de-duplicating empty</span>
      <span class="c1"># container types which compare equal.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">((</span><span class="n">layer</span> <span class="ow">is</span> <span class="n">value</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_use_resource_variables&#39;</span><span class="p">):</span>
          <span class="c1"># Legacy layers (V1 tf.layers) must always use</span>
          <span class="c1"># resource variables.</span>
          <span class="n">value</span><span class="o">.</span><span class="n">_use_resource_variables</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Append value to list of trainable / non-trainable weights if relevant</span>
    <span class="c1"># TODO(b/125122625): This won&#39;t pick up on any variables added to a</span>
    <span class="c1"># list/dict after creation.</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
      <span class="c1"># TODO(b/126450014): Remove `_UnreadVariable` check here when assign ops</span>
      <span class="c1"># no longer return True for isinstance Variable checks.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">_UnreadVariable</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">continue</span>

      <span class="c1"># Users may add extra weights/variables</span>
      <span class="c1"># simply by assigning them to attributes (invalid for graph networks)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
      <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">val</span> <span class="ow">is</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="p">):</span>
          <span class="k">continue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">val</span> <span class="ow">is</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="p">):</span>
          <span class="k">continue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

      <span class="n">backend</span><span class="o">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="c1"># Skip the auto trackable from tf.Module to keep status quo. See the comment</span>
    <span class="c1"># at __delattr__.</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_gather_children_attribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attribute</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="p">{</span>
        <span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="s1">&#39;trainable_weights&#39;</span><span class="p">,</span> <span class="s1">&#39;non_trainable_weights&#39;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">):</span>
      <span class="n">nested_layers</span> <span class="o">=</span> <span class="n">layer_utils</span><span class="o">.</span><span class="n">filter_empty_layer_containers</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span>
          <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span>
              <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">attribute</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">nested_layers</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">_flatten_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">include_self</span><span class="p">:</span>
      <span class="k">yield</span> <span class="bp">self</span>

    <span class="c1"># Only instantiate set and deque if needed.</span>
    <span class="n">layers_or_containers</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">layers_or_containers</span><span class="p">:</span>
      <span class="n">seen_object_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
      <span class="n">deque</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">layers_or_containers</span><span class="p">)</span>
      <span class="k">while</span> <span class="n">deque</span><span class="p">:</span>
        <span class="n">layer_or_container</span> <span class="o">=</span> <span class="n">deque</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>

        <span class="n">layer_or_container_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">layer_or_container</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">layer_or_container_id</span> <span class="ow">in</span> <span class="n">seen_object_ids</span><span class="p">:</span>
          <span class="k">continue</span>
        <span class="n">seen_object_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layer_or_container_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_or_container</span><span class="p">,</span> <span class="n">Layer</span><span class="p">)</span> <span class="ow">and</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_or_container</span><span class="p">,</span> <span class="n">metrics_mod</span><span class="o">.</span><span class="n">Metric</span><span class="p">)):</span>
          <span class="k">yield</span> <span class="n">layer_or_container</span>
          <span class="c1"># Introspect recursively through sublayers.</span>
          <span class="k">if</span> <span class="n">recursive</span><span class="p">:</span>
            <span class="n">sublayers</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layer_or_container</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">sublayers</span><span class="p">:</span>
              <span class="n">deque</span><span class="o">.</span><span class="n">extendleft</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">sublayers</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_or_container</span><span class="p">,</span>
                        <span class="n">data_structures</span><span class="o">.</span><span class="n">TrackableDataStructure</span><span class="p">):</span>
          <span class="c1"># Data structures are introspected even with `recursive=False`.</span>
          <span class="n">tracked_values</span> <span class="o">=</span> <span class="n">layer_or_container</span><span class="o">.</span><span class="n">_values</span>
          <span class="k">if</span> <span class="n">tracked_values</span><span class="p">:</span>
            <span class="n">deque</span><span class="o">.</span><span class="n">extendleft</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">tracked_values</span><span class="p">))</span>

  <span class="c1"># This is a hack so that the is_layer (within</span>
  <span class="c1"># training/trackable/layer_utils.py) check doesn&#39;t get the weights attr.</span>
  <span class="c1"># TODO(b/110718070): Remove when fixed.</span>
  <span class="k">def</span> <span class="nf">_is_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_init_call_fn_args</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Clear cached call function arguments.</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">_call_full_argspec</span><span class="o">.</span><span class="n">fget</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">_call_fn_args</span><span class="o">.</span><span class="n">fget</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">_call_accepts_kwargs</span><span class="o">.</span><span class="n">fget</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">call_fn_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;training&#39;</span> <span class="ow">in</span> <span class="n">call_fn_args</span> <span class="ow">or</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">_call_accepts_kwargs</span><span class="p">)</span>
    <span class="c1"># The default training arg will be any (non-None) default specified in the</span>
    <span class="c1"># method signature, or None if no value is specified.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_training_arg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_arg_defaults</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s1">&#39;training&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_mask_arg</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;mask&#39;</span> <span class="ow">in</span> <span class="n">call_fn_args</span> <span class="ow">or</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">_call_accepts_kwargs</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@layer_utils</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_call_full_argspec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Argspec inspection is expensive and the call spec is used often, so it</span>
    <span class="c1"># makes sense to cache the result.</span>
    <span class="k">return</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@layer_utils</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_call_fn_args</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">all_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_full_argspec</span><span class="o">.</span><span class="n">args</span>
    <span class="c1"># Scrub `self` that appears if a decorator was applied.</span>
    <span class="k">if</span> <span class="n">all_args</span> <span class="ow">and</span> <span class="n">all_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;self&#39;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">all_args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">all_args</span>

  <span class="nd">@property</span>
  <span class="nd">@layer_utils</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_call_fn_arg_defaults</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">call_fn_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span>
    <span class="n">call_fn_defaults</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_full_argspec</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">or</span> <span class="p">[]</span>
    <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="c1"># The call arg defaults are an n-tuple of the last n elements of the args</span>
    <span class="c1"># list. (n = # of elements that have a default argument)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_fn_defaults</span><span class="p">),</span> <span class="mi">0</span><span class="p">):</span>
      <span class="n">defaults</span><span class="p">[</span><span class="n">call_fn_args</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">call_fn_defaults</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">defaults</span>

  <span class="nd">@property</span>
  <span class="nd">@layer_utils</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_call_fn_arg_positions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">call_fn_arg_positions</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span><span class="p">):</span>
      <span class="n">call_fn_arg_positions</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos</span>
    <span class="k">return</span> <span class="n">call_fn_arg_positions</span>

  <span class="nd">@property</span>
  <span class="nd">@layer_utils</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_call_accepts_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_full_argspec</span><span class="o">.</span><span class="n">varkw</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_eager_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># A list of loss values containing activity regularizers and losses</span>
    <span class="c1"># manually added through `add_loss` during eager execution. It is cleared</span>
    <span class="c1"># after every batch.</span>
    <span class="c1"># Because we plan on eventually allowing a same model instance to be trained</span>
    <span class="c1"># in eager mode or graph mode alternatively, we need to keep track of</span>
    <span class="c1"># eager losses and symbolic losses via separate attributes.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s1">&#39;_eager_losses&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_eager_losses</span>

  <span class="nd">@_eager_losses</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_eager_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="n">losses</span>

  <span class="k">def</span> <span class="nf">_dedup_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dedupe weights while maintaining order as much as possible.&quot;&quot;&quot;</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">seen_ids</span> <span class="o">=</span> <span class="p">[],</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="c1"># Track the Variable&#39;s identity to avoid __eq__ issues.</span>
        <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">output</span>

  <span class="k">def</span> <span class="nf">_split_out_first_arg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Grab the argument corresponding to the first argument in the</span>
    <span class="c1"># layer&#39;s `call` method spec. This will either be the first positional</span>
    <span class="c1"># argument, or it will be provided as a keyword argument.</span>
    <span class="k">if</span> <span class="n">args</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="n">kwargs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;The first argument to `Layer.call` must always be passed.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>

  <span class="c1"># SavedModel properties. Please see keras/saving/saved_model for details.</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_set_save_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_saved_model_inputs_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>  <span class="c1"># Already set.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_saved_model_inputs_spec</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">tf_utils</span><span class="o">.</span><span class="n">get_tensor_spec</span><span class="p">,</span>
                                                       <span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_save_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dynamic_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_saved_model_inputs_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_tensor_spec</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dynamic_batch</span><span class="o">=</span><span class="n">dynamic_batch</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_saved_model_inputs_spec</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_trackable_saved_model_saver</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">layer_serialization</span><span class="o">.</span><span class="n">LayerSavedModelSaver</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_object_identifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saved_model_saver</span><span class="o">.</span><span class="n">object_identifier</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_tracking_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saved_model_saver</span><span class="o">.</span><span class="n">tracking_metadata</span>

  <span class="k">def</span> <span class="nf">_list_extra_dependencies_for_serialization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">serialization_cache</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saved_model_saver</span>
            <span class="o">.</span><span class="n">list_extra_dependencies_for_serialization</span><span class="p">(</span><span class="n">serialization_cache</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_list_functions_for_serialization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">serialization_cache</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saved_model_saver</span>
            <span class="o">.</span><span class="n">list_functions_for_serialization</span><span class="p">(</span><span class="n">serialization_cache</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Override to support `copy.deepcopy` and pickling.</span>
    <span class="c1"># Thread-local objects cannot be copied in Python 3, so pop these.</span>
    <span class="c1"># Thread-local objects are used to cache losses in MirroredStrategy, and</span>
    <span class="c1"># so shouldn&#39;t be copied.</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;_thread_local&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;_metrics_lock&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span>

  <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_thread_local&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>
    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_metrics_lock&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="c1"># Bypass Trackable logic as `__dict__` already contains this info.</span>
    <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;__dict__&#39;</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TensorFlowOpLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a TensorFlow Operation in a Layer.</span>

<span class="sd">  This class is used internally by the Functional API. When a user</span>
<span class="sd">  uses a raw TensorFlow Operation on symbolic tensors originating</span>
<span class="sd">  from an `Input` Layer, the resultant operation will be wrapped</span>
<span class="sd">  with this Layer object in order to make the operation compatible</span>
<span class="sd">  with the Keras API.</span>

<span class="sd">  This Layer will create a new, identical operation (except for inputs</span>
<span class="sd">  and outputs) every time it is called. If `run_eagerly` is `True`,</span>
<span class="sd">  the op creation and calculation will happen inside an Eager function.</span>

<span class="sd">  Instances of this Layer are created when `autolambda` is called, which</span>
<span class="sd">  is whenever a Layer&#39;s `__call__` encounters symbolic inputs that do</span>
<span class="sd">  not have Keras metadata, or when a Network&#39;s `__init__` encounters</span>
<span class="sd">  outputs that do not have Keras metadata.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    node_def: String, the serialized NodeDef of the Op this layer will wrap.</span>
<span class="sd">    name: String, the name of the Layer.</span>
<span class="sd">    constants: Dict of NumPy arrays, the values of any Tensors needed for this</span>
<span class="sd">      Operation that do not originate from a Keras `Input` Layer. Since all</span>
<span class="sd">      placeholders must come from Keras `Input` Layers, these Tensors must be</span>
<span class="sd">      treated as constant in the Functional API.</span>
<span class="sd">    trainable: Bool, whether this Layer is trainable. Currently Variables are</span>
<span class="sd">      not supported, and so this parameter has no effect.</span>
<span class="sd">    dtype: The default dtype of this Layer. Inherited from `Layer` and has no</span>
<span class="sd">      effect on this class, however is used in `get_config`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">node_def</span><span class="p">,</span>
               <span class="n">name</span><span class="p">,</span>
               <span class="n">constants</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Pass autocast=False, as if inputs are cast, input types might not match</span>
    <span class="c1"># Operation type.</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TensorFlowOpLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">_TF_OP_LAYER_NAME_PREFIX</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">autocast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_def</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span> <span class="o">=</span> <span class="n">json_format</span><span class="o">.</span><span class="n">ParseDict</span><span class="p">(</span><span class="n">node_def</span><span class="p">,</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_def</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">):</span>
        <span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="o">.</span><span class="n">FromString</span><span class="p">(</span><span class="n">node_def</span><span class="p">)</span>
    <span class="c1"># JSON serialization stringifies keys which are integer input indices.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">constants</span> <span class="o">=</span> <span class="p">({</span>
        <span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">):</span> <span class="n">constant</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">constant</span> <span class="ow">in</span> <span class="n">constants</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span> <span class="k">if</span> <span class="n">constants</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{})</span>
    <span class="c1"># Layer uses original op unless it is called on new inputs.</span>
    <span class="c1"># This means `built` is not set in `__call__`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Do not individually trace TensorflowOpLayers in the SavedModel.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_must_restore_from_config</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_defun_call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_op</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_make_node_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
    <span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">()</span>
    <span class="n">node_def</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>
    <span class="c1"># Used in TPUReplicateContext to indicate whether this node has been cloned</span>
    <span class="c1"># and to not add TPU attributes.</span>
    <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;_cloned&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">node_def</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">node_def</span>

  <span class="k">def</span> <span class="nf">_make_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">graph</span>
    <span class="n">node_def</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_node_def</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">constant</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Recreate constant in graph to add distribution context.</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">constant</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">constant</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">node_def</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">constant</span><span class="p">)</span>
      <span class="n">c_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">_create_c_op</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">control_inputs</span><span class="o">=</span><span class="p">[])</span>
      <span class="n">op</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">_create_op_from_tf_operation</span><span class="p">(</span><span class="n">c_op</span><span class="p">)</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_control_flow_post_processing</span><span class="p">()</span>

      <span class="c1"># Record the gradient because custom-made ops don&#39;t go through the</span>
      <span class="c1"># code-gen&#39;d eager call path</span>
      <span class="n">op_type</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">op_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">attr_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">attr</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">op_def</span><span class="o">.</span><span class="n">attr</span><span class="p">]</span>
      <span class="n">attrs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">attr_names</span><span class="p">:</span>
        <span class="n">attrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attr_name</span><span class="p">)</span>
        <span class="n">attrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">))</span>
      <span class="n">attrs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">attrs</span><span class="p">)</span>
      <span class="n">execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span>

  <span class="nd">@def_function</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">_defun_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps the op creation method in an Eager function for `run_eagerly`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_op</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">TensorFlowOpLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="c1"># `__init__` prefixes the name. Revert to the constructor argument.</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">_TF_OP_LAYER_NAME_PREFIX</span><span class="p">):],</span>
        <span class="s1">&#39;node_def&#39;</span><span class="p">:</span> <span class="n">json_format</span><span class="o">.</span><span class="n">MessageToDict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="p">),</span>
        <span class="s1">&#39;constants&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="n">i</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">config</span>


<span class="k">class</span> <span class="nc">AddLoss</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds its inputs as a loss.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    unconditional: Whether or not the loss should be conditioned on the inputs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unconditional</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Pass autocast=False, as there is no reason to cast loss to a different</span>
    <span class="c1"># dtype.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;autocast&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AddLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unconditional</span> <span class="o">=</span> <span class="n">unconditional</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">inputs</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AddLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;unconditional&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">config</span>


<span class="k">class</span> <span class="nc">AddMetric</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds its inputs as a metric.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    aggregation: &#39;mean&#39; or None. How the inputs should be aggregated.</span>
<span class="sd">    metric_name: The name to use for this metric.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AddMetric</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">metric_name</span> <span class="o">=</span> <span class="n">metric_name</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AddMetric</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;aggregation&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="s1">&#39;metric_name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_name</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">config</span>


<span class="k">def</span> <span class="nf">_in_functional_construction_mode</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">input_list</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
  <span class="sd">&quot;&quot;&quot;Check the arguments to see if we are constructing a functional model.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">keras_tensors_enabled</span><span class="p">():</span>
    <span class="c1"># We are constructing a functional model if any of the inputs</span>
    <span class="c1"># are KerasTensors</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">keras_tensor</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">all_inputs_symbolic</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
          <span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">)</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_subclassed</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="ow">and</span>
          <span class="nb">any</span><span class="p">(</span><span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span>
              <span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">all_inputs_symbolic</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;It appears you are trying to construct a &#39;</span>
                         <span class="s1">&#39;functional model, but not all of the inputs in &#39;</span>
                         <span class="s1">&#39;the first positional argument of your layer call &#39;</span>
                         <span class="s1">&#39;are symbolic tensors. &#39;</span>
                         <span class="s1">&#39;(Input objects, or the output of another layer) &#39;</span>
                         <span class="s1">&#39;Functional models cannot correctly track custom &#39;</span>
                         <span class="s1">&#39;layers unless all values in the first call argument &#39;</span>
                         <span class="s1">&#39;are symbolic.&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">all_inputs_symbolic</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_keras_graph</span><span class="p">()</span> <span class="ow">or</span>
              <span class="nb">all</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;_keras_history&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_convert_numpy_or_python_types</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">np_arrays</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_v2_with_dispatch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span>


<span class="c1"># Avoid breaking users who directly import this symbol from this file.</span>
<span class="c1"># TODO(fchollet): remove this.</span>
<span class="n">InputSpec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span>  <span class="c1"># pylint:disable=invalid-name</span>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2020, Jimmy.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.4.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>